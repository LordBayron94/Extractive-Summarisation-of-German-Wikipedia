{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b11a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, treetaggerwrapper, networkx as nx, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk, string\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "nltk.download('wordnet')\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b38a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_class.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9797badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet') # first-time use only\n",
    "german_stop_words = stopwords.words('german')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5f89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['al', 'au', 'bi', 'de', 'diesis', 'dy', 'e', 'mus', 'un', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "tv = TfidfVectorizer(min_df = 0.05, max_df = 0.8, tokenizer=LemNormalize, stop_words = stopword_list)\n",
    "X = tv.fit_transform(df['source1'])\n",
    "vocab = tv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a237ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.DataFrame(X.toarray(), columns = vocab)\n",
    "X1['no_words_inSent_SS\t'] = df['no_words_inSent_SS']\n",
    "X1['no_words_inSent_SK'] = df['no_words_inSent_SK']\n",
    "X1['noCap_LetterWords_inSentence'] = df['noCap_LetterWords_inSentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dba8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse = sparse.csr_matrix(X1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e71b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3513935, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cf4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_sparse.toarray()\n",
    "y = df.sim_sent.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893970a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36526d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Naive Bayes\n",
      "---- confusion_matrix:\n",
      "[[     4 515678]\n",
      " [     2 538497]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "predict = clf.predict(X_test)\n",
    "print(\"\\n============ Naive Bayes\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25acd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.00      0.00    515682\n",
      "           1       0.51      1.00      0.68    538499\n",
      "\n",
      "    accuracy                           0.51   1054181\n",
      "   macro avg       0.59      0.50      0.34   1054181\n",
      "weighted avg       0.59      0.51      0.35   1054181\n",
      "\n",
      "---- accuracy_score:\n",
      "0.5108240425505677\n"
     ]
    }
   ],
   "source": [
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,predict))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f72c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Naive Bayes\n",
      "---- confusion_matrix:\n",
      "[[ 15474 500208]\n",
      " [ 16450 522049]]\n",
      "---- classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.03      0.06    515682\n",
      "           1       0.51      0.97      0.67    538499\n",
      "\n",
      "    accuracy                           0.51   1054181\n",
      "   macro avg       0.50      0.50      0.36   1054181\n",
      "weighted avg       0.50      0.51      0.37   1054181\n",
      "\n",
      "---- accuracy_score:\n",
      "0.5098963081292491\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(X_train, y_train)\n",
    "y_pred = gnb_classifier.predict(X_test)\n",
    "print(\"\\n============ Naive Bayes\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "076d3e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Random Forest\n",
      "---- confusion_matrix:\n",
      "[[     3 515679]\n",
      " [     2 538497]]\n",
      "---- classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.00      0.00    515682\n",
      "           1       0.51      1.00      0.68    538499\n",
      "\n",
      "    accuracy                           0.51   1054181\n",
      "   macro avg       0.56      0.50      0.34   1054181\n",
      "weighted avg       0.55      0.51      0.35   1054181\n",
      "\n",
      "---- accuracy_score:\n",
      "0.5108230939468649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_classifier = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = forest_classifier.predict(X_test)\n",
    "print(\"\\n============ Random Forest\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(gamma='scale')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(\"\\n============ Support Vector Machine\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "plt.title(\"Distribution in Summary important sentences Class Labels\")\n",
    "plt.bar(dict(Counter(df['sim_sent'])).keys(), dict(Counter(df['sim_sent'])).values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
