{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b1ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, product\n",
    "import warnings, pandas as pd, nltk, string\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim, string\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import kullback_leibler, hellinger\n",
    "from gensim.models import ldamodel\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity, Similarity\n",
    "#from gensim.summarization.bm25 import get_bm25_weights\n",
    "from gensim.utils import simple_preprocess\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import spacy,sys,nltk\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "import networkx as nx, algorithmx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfcab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e793ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1, s2):\n",
    "    s1 = set(s1); s2=set(s2)\n",
    "    return nltk.jaccard_distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ca51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbke_score(df, top_n, thresh):\n",
    "    sentences = sent_tokenize(df, language=\"german\")\n",
    "    g=nx.DiGraph()\n",
    "    edges = [(i,j,jaccard(x,y)) \n",
    "             for i,x in enumerate(sentences) \n",
    "             for j,y in enumerate(sentences) if i < j]\n",
    "\n",
    "    g.add_weighted_edges_from((i,j,sim) for i,j,sim in edges)\n",
    "    lst =[]\n",
    "    for i in g.nodes():\n",
    "        no_nei = len(g.edges(i))\n",
    "        if no_nei !=0 :\n",
    "            w_sum = g.degree(i,weight='weight')\n",
    "            w_sum += w_sum\n",
    "            alpha = 1-w_sum\n",
    "            sbke = no_nei*(w_sum/no_nei)**alpha\n",
    "            lst.append((sentences[i], sbke))\n",
    "        else:\n",
    "            lst.append((sentences[i], 0))\n",
    "\n",
    "    top = nlargest(top_n, lst, key=itemgetter(1))\n",
    "    top_s = [x[0] for x in top]\n",
    "    string = ' '.join([str(e) for e in top_s])\n",
    "    return string\n",
    "\n",
    "test_df = df.head(50)\n",
    "test_df['sbke_jacc1'] = test_df.apply(lambda x: sbke_score(x['source'], 1, 0.15), axis=1)\n",
    "test_df['sbke_jacc2'] = test_df.apply(lambda x: sbke_score(x['source'], 2, 0.15), axis=1)\n",
    "test_df['sbke_jacc5'] = test_df.apply(lambda x: sbke_score(x['source'], 5, 0.15), axis=1)\n",
    "test_df['sbke_jacc10'] = test_df.apply(lambda x: sbke_score(x['source'], 10, 0.15), axis=1)\n",
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/sbke_cos.csv', encoding='utf-8')\n",
    "\n",
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/sbke_jacc_res.txt\", \"w\")\n",
    "\n",
    "hy, rf = test_df['sbke_jacc1'], test_df['summary']\n",
    "rouge= Rouge()\n",
    "scoresT1= rouge.get_scores(hy, rf, avg=True)\n",
    "print(\"sbke_jacc1 scores: \", scoresT1, '\\n\\n')\n",
    "\n",
    "hy2 = test_df['sbke_jacc2']\n",
    "scoresT2= rouge.get_scores(hy2, rf, avg=True)\n",
    "print(\"sbke_jacc2 scores: \", scoresT2, '\\n\\n')\n",
    "\n",
    "hy5 = test_df['sbke_jacc5']\n",
    "scoresT5= rouge.get_scores(hy5, rf, avg=True)\n",
    "print(\"sbke_jacc5 scores: \", scoresT5, '\\n\\n')\n",
    "\n",
    "hy10 = test_df['sbke_jacc10']\n",
    "scoresT10= rouge.get_scores(hy10, rf, avg=True)\n",
    "print(\"sbke_jacc10 scores: \", scoresT10, '\\n\\n')\n",
    "\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc52e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_Score(df, top_n, thresh):\n",
    "    sentences = sent_tokenize(df, language=\"german\")\n",
    "    g=nx.DiGraph()\n",
    "    edges = [(i,j,jaccard(x,y)) \n",
    "             for i,x in enumerate(sentences) \n",
    "             for j,y in enumerate(sentences) if i < j]\n",
    "\n",
    "    g.add_weighted_edges_from((i,j,sim) for i,j,sim in edges)\n",
    "    lst,res = [],[]\n",
    "    pr = nx.pagerank(g, weight='weight')\n",
    "    \n",
    "    for s, i in enumerate(sentences):\n",
    "        lst.append((i, pr[s]))\n",
    "    top = nlargest(top_n, lst, key=itemgetter(1))\n",
    "    top_s = [x[0] for x in top]\n",
    "    string = ' '.join([str(e) for e in top_s])\n",
    "    return (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.head(50)\n",
    "test_df['lex_jacc1'] = test_df.apply(lambda x: lex_Score(x['source'], 1, 0.15), axis=1)\n",
    "test_df['lex_jacc2'] = test_df.apply(lambda x: lex_Score(x['source'], 2, 0.15), axis=1)\n",
    "test_df['lex_jacc5'] = test_df.apply(lambda x: lex_Score(x['source'], 5, 0.15), axis=1)\n",
    "test_df['lex_jacc10'] = test_df.apply(lambda x: lex_Score(x['source'], 10, 0.15), axis=1)\n",
    "test_df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/lex_miha.csv', encoding='utf-8')\n",
    "from rouge import Rouge\n",
    "import sys\n",
    "\n",
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/lex_jacc_res.txt\", \"w\")\n",
    "\n",
    "hy, rf = test_df['lex_jacc1'], test_df['summary']\n",
    "rouge= Rouge()\n",
    "scoresT1= rouge.get_scores(hy, rf, avg=True)\n",
    "print(\"lex_jacc1 scores: \", scoresT1, '\\n\\n')\n",
    "\n",
    "hy2 = test_df['lex_jacc2']\n",
    "scoresT2= rouge.get_scores(hy2, rf, avg=True)\n",
    "print(\"lex_jacc2 scores: \", scoresT2, '\\n\\n')\n",
    "\n",
    "hy5 = test_df['lex_jacc5']\n",
    "scoresT5= rouge.get_scores(hy5, rf, avg=True)\n",
    "print(\"lex_jacc5 scores: \", scoresT5, '\\n\\n')\n",
    "\n",
    "hy10 = test_df['lex_jacc10']\n",
    "scoresT10= rouge.get_scores(hy10, rf, avg=True)\n",
    "print(\"lex_jacc10 scores: \", scoresT10, '\\n\\n')\n",
    "\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36942d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
