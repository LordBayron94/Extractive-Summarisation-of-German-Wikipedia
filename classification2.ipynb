{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55589d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, treetaggerwrapper, networkx as nx, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk, string\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "nltk.download('wordnet')\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98a2f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_features.csv', encoding='utf-8')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f210aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_lst(df):\n",
    "    lst = []\n",
    "    for i in lst:\n",
    "        spl = i.split(',')\n",
    "        lst.append(spl)\n",
    "    return\n",
    "        \n",
    "\n",
    "data.sim_sent = data.sim_sent.apply(lst_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11b63120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(data.sim_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da867bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['source']\n",
    "y = data['sim_sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f879cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153007d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = nltk.corpus.stopwords.words('german')\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7, stop_words=german_stop_words)\n",
    "X = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7840a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=250, random_state=35)\n",
    "classifier.fit(X_train.astype(int), y_train.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return tokens\n",
    "data.source = data.source.apply(identify_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba004e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tur_to_arr(df):\n",
    "    return np.array(df)\n",
    "data.sim_sent = data.sim_sent.apply(tur_to_arr)\n",
    "data.source = data.source.apply(tur_to_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['source'][0]\n",
    "y = data['sim_sent'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3314cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f439953",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "X_train = np.array(data.source[0])\n",
    "y_train = np.array(data.sim_sent[0])\n",
    "\n",
    "X_test = np.array(data.source[5])   \n",
    "\n",
    "\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(min_df=1,max_df=2)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def capital_lWords(df):\n",
    "    return np.array([sum([c.isupper() for c in a]) for a in df]).reshape(-1,1)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ])),\n",
    "        ('capital', Pipeline([\n",
    "            ('count', FunctionTransformer(capital_lWords, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=10, random_state=0))])\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dba551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def capital_lWords(df):\n",
    "    return np.array([sum([c.isupper() for c in a]) for a in df]).reshape(-1,1)\n",
    "\n",
    "classifier2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(max_features=100, min_df=5, max_df=0.7)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ])),\n",
    "        ('capital', Pipeline([\n",
    "            ('count', FunctionTransformer(capital_lWords, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', svm.SVC())])\n",
    "classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "dense_matrix = X.todense()\n",
    "np.insert(dense_matrix,dense_matrix.shape[1],[val1,...,valN],axis=1)\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def capital_lWords(df):\n",
    "    return np.array([sum([c.isupper() for c in a]) for a in df]).reshape(-1,1)\n",
    "\n",
    "classifier3 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(max_features=100, min_df=5, max_df=0.7)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ])),\n",
    "        ('capital', Pipeline([\n",
    "            ('count', FunctionTransformer(capital_lWords, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', GaussianNB())])\n",
    "classifier3.fit(X_train, y_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fdbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a351664",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = classifier3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628664a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import sys\n",
    "\n",
    "#sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/rand_f_tfidf.txt\", \"w\")\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred), \"\\n\\n\")\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "#sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/naive_b_tfidf.txt\", \"w\")\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred), \"\\n\\n\")\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred3, average='weighted')))\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/svm_tfidf.txt\", \"w\")\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred), \"\\n\\n\")\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred2, average='weighted')))\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8481559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba698a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_lisnltk.corpus.stopwords.wordst = ('german')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in nltk.word_tokenize(text) if word not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "df[\"source\"] = df[\"source\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85,stop_words=german_stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(df['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538133a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_\n",
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14522a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30715a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kew(df):\n",
    "    kw=[]\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([df]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    for k in keywords:\n",
    "        kw.append(k)\n",
    "    return kw\n",
    "\n",
    "df['keywords'] = df['source'].apply(get_kew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4503b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_lWords(df):\n",
    "    tokens = sent_tokenize(df, language=\"german\")\n",
    "    return [sum([c.isupper() for c in a]) for a in tokens]\n",
    "\n",
    "def W_sourceSummary(df, df2):\n",
    "    f = []\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len(list((set(s.split()).intersection(set(tok2))))))\n",
    "    return f\n",
    "\n",
    "def W_sourceKeywords(df, df2):\n",
    "    f = []\n",
    "    df.lower()\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len(list((set(s.split()).intersection(set(tok2))))))\n",
    "    return f\n",
    "\n",
    "def listToString(df):\n",
    "    string = ' '.join([str(e) for e in df])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_words_inSent_SS'] = df.apply(lambda x: W_sourceSummary(x['source'], x['summary']), axis=1)\n",
    "df['keywords'] = df['keywords'].apply(listToString)\n",
    "df['no_words_inSent_SK'] = df.apply(lambda x: W_sourceKeywords(x['source'], x['keywords']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet') # first-time use only\n",
    "german_stop_words = stopwords.words('german')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=german_stop_words)\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "\n",
    "def get_docs(df):\n",
    "    threshold = 0.15\n",
    "    doc = sent_tokenize(df, language=\"german\")\n",
    "    lst, tags, output_tuples, similar_sent = [], [], [], [] \n",
    "    arr = cos_similarity(doc)\n",
    "    doc_names = [\"s\"+str(s) for s in range(0, len(doc), 1)]\n",
    "    for row in range(len(arr)):\n",
    "        lst = [row+1+idx for idx, num in \\\n",
    "               \n",
    "                  enumerate(arr[row, row+1:]) if num >= threshold]\n",
    "        [(output_tuples.append((doc[row], doc[item]))) for item in lst]\n",
    "    for tup in output_tuples:\n",
    "        for sent in tup:\n",
    "            similar_sent.append(sent)\n",
    "    for sent in doc:\n",
    "        if sent in set(doc).intersection(set(similar_sent)):\n",
    "            tags.append(1)\n",
    "        else:\n",
    "            tags.append(0)\n",
    "    return tags\n",
    "#get_docs(df['source'][0])\n",
    "df['sim_sent'] = df['source'].apply(get_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62423a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.drop(['summary', 'keywords'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_lWords(df):\n",
    "    tokens = sent_tokenize(df, language=\"german\")\n",
    "    return list([sum([c.isupper() for c in a]) for a in tokens])\n",
    "\n",
    "dat['noCap_LetterWords_inSentence'] = dat['source'].apply(capital_lWords)\n",
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_features.csv')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return np.array(tokens)\n",
    "dat['source'] = dat['source'].apply(identify_tokens)\n",
    "dat['source'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d246495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df1 = dat.iloc[0, 0:2]\n",
    "\n",
    "X = dat['source']\n",
    "y = dat['sim_sent']\n",
    "\n",
    "#X_test = np.array([\"it's a nice day in nyc\",\n",
    "#                   'i loved the time i spent in london, the weather was great, though there was a nip in the air and i had to wear a jacket.'\n",
    "#                   ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\\\n",
    "                                                 y, test_size=0.3, random_state=25)\n",
    "\n",
    "\n",
    "\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1,1)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(min_df=1,max_df=2)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 100))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4531eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "X = dat['source']\n",
    "y = dat['sim_sent']\n",
    "\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train.astype(int), y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed035f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "x = np.array(dat['source'])\n",
    "\n",
    "y = np.array(dat['sim_sent'])\n",
    "print(y_train.shape, X_train.shape)\n",
    "print(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
