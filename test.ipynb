{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67f01cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: treetaggerwrapper in h:\\anaconda\\lib\\site-packages (2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HanTa in h:\\anaconda\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install treetaggerwrapper\n",
    "import os, treetaggerwrapper, networkx as nx\n",
    "!pip install HanTa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957b8cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>Anthony Minghella, CBE war ein britischer Film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>Die Auteur-Theorie ist eine Filmtheorie und di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>Alfredo James \"Al\" Pacino ist ein US-amerikani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>Als Alkalimetalle werden die chemischen Elemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>Das deutsche Arbeitsrecht ist ein Rechtsgebiet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Minghella war der Sohn italienisch-schottische...   \n",
       "1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                             summary  \n",
       "0  Anthony Minghella, CBE war ein britischer Film...  \n",
       "1  Die Auteur-Theorie ist eine Filmtheorie und di...  \n",
       "2  Alfredo James \"Al\" Pacino ist ein US-amerikani...  \n",
       "3  Als Alkalimetalle werden die chemischen Elemen...  \n",
       "4  Das deutsche Arbeitsrecht ist ein Rechtsgebiet...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text)]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "df['text_lemmatized'] = df['source'].apply(lemmatize_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    sent_tokenize = nltk.tokenize.word_tokenize(text, language='german')\n",
    "    tags = lemmatizer.tag_sent(sent_tokenize)\n",
    "    return tags\n",
    "\n",
    "#df['text_lemmatized'] = df['source'].apply(lemmatize_text)\n",
    "\n",
    "print(lemmatize_text(df['word_token'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c770fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tree_tagger = treetaggerwrapper.TreeTagger(TAGLANG='de')\n",
    "    tags = tree_tagger.tag_text(tokenized_sentance, tagonly=True)\n",
    "    return tags\n",
    "\n",
    "print(lemmatize_text(df['source'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text, language='german')\n",
    "    tags = [ps.stem(w) for w in words]\n",
    "    return tags\n",
    "\n",
    "#df['text_lemmatized'] = df['source'].apply(lemmatize_text)\n",
    "\n",
    "print(stem_text(df['source'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    ps = GermanStemmer()\n",
    "    words = word_tokenize(text, language='german')\n",
    "    tags = [ps.stem(w) for w in words]\n",
    "    return tags\n",
    "\n",
    "#df['text_lemmatized'] = df['source'].apply(lemmatize_text)\n",
    "\n",
    "print(stem_text(df['source'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return tokens\n",
    "\n",
    "df['sent_token'] = df['source'].apply(identify_tokens)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = word_tokenize(row, language='german')\n",
    "    return tokens\n",
    "\n",
    "df['word_token'] = df['source'].apply(identify_tokens)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in nltk.word_tokenize(text) if word not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "df[\"filtered_text\"] = df[\"text_lemmatized\"].apply(remove_stopwords)\n",
    "#df.head()\n",
    "\n",
    "print(remove_stopwords(df['source'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdf_model = []\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=0.1,\n",
    "                                 lowercase=True)\n",
    "tfIdf_model = vectorizer.fit_transform(df['filtered_text'])\n",
    "print (tfIdf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07144d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text_lemmatized'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220f9969",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d87464127b18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_sets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'avd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tai'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msource_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msingle_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'single_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0)\n",
    "\n",
    "tfidf_matrices = []\n",
    "data_sets = []\n",
    "for source in ['avd', 'tai', 'tds']:\n",
    "    source_data = single_matrix[single_matrix['source'] == source].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a14e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=2)\n",
    "km_model.fit(tfidf_model)\n",
    " \n",
    "clustering = collections.defaultdict(list)\n",
    "for idx, label in enumerate(km_model.labels_):\n",
    "    clustering[label].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"source\"], df['filtered_text'], test_size=0.30, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "bow_train_features = vectorizer.fit_transform(X_train)\n",
    "bow_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(bow_train_features,y_train)\n",
    "\n",
    "predictions = mnb.predict(bow_test_features)\n",
    "\n",
    "score = mnb.score(bow_test_features, y_test)\n",
    "print(\"Bag of words accuracy \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "mat_with_cosine_sim  = []\n",
    "for m in tfIdf_model:\n",
    "    mat_with_cosine_sim.append(linear_kernel(m, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cef3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.         0.02597783 0.02662211\n",
      "  0.07371142 0.         0.01884577 0.01209965 0.         0.03562628\n",
      "  0.04853386 0.02940111 0.11751224 0.06874723 0.02339968 0.03799937\n",
      "  0.04485464 0.05478277 0.11043322 0.         0.26564482]\n",
      " [0.         1.         0.         0.1810938  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12421882 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.1810938  0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14517853 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10554072]\n",
      " [0.02597783 0.         0.         0.         1.         0.14553316\n",
      "  0.10245552 0.         0.         0.04308864 0.         0.09798114\n",
      "  0.04150724 0.         0.         0.         0.         0.01709567\n",
      "  0.03836069 0.04685144 0.05683975 0.07013591 0.        ]\n",
      " [0.02662211 0.         0.         0.         0.14553316 1.\n",
      "  0.12266036 0.         0.         0.         0.         0.03122405\n",
      "  0.04253667 0.         0.         0.         0.         0.01751966\n",
      "  0.03931208 0.04801342 0.08372159 0.         0.        ]\n",
      " [0.07371142 0.         0.         0.         0.10245552 0.12266036\n",
      "  1.         0.         0.03749232 0.0407543  0.         0.08645327\n",
      "  0.11777572 0.         0.         0.         0.         0.04850852\n",
      "  0.18924327 0.13293977 0.05376044 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.01884577 0.         0.         0.         0.         0.\n",
      "  0.03749232 0.         1.         0.00833231 0.         0.\n",
      "  0.         0.02024679 0.08092366 0.04734211 0.01611396 0.01240216\n",
      "  0.14430459 0.08359355 0.02198288 0.         0.01936965]\n",
      " [0.01209965 0.         0.         0.         0.04308864 0.\n",
      "  0.0407543  0.         0.00833231 1.         0.034051   0.\n",
      "  0.02892613 0.01299915 0.02081377 0.0303953  0.01034573 0.05561808\n",
      "  0.         0.         0.07288362 0.         0.012436  ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.034051   1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.03562628 0.         0.         0.         0.09798114 0.03122405\n",
      "  0.08645327 0.         0.         0.         0.         1.\n",
      "  0.05692349 0.         0.         0.         0.         0.02344519\n",
      "  0.05260828 0.06425259 0.         0.09618518 0.        ]\n",
      " [0.04853386 0.         0.         0.         0.04150724 0.04253667\n",
      "  0.11777572 0.         0.         0.02892613 0.         0.05692349\n",
      "  1.         0.         0.         0.         0.         0.0319395\n",
      "  0.07166851 0.08753163 0.         0.         0.        ]\n",
      " [0.02940111 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.02024679 0.01299915 0.         0.\n",
      "  0.         1.         0.05057567 0.07385796 0.02513924 0.01934848\n",
      "  0.         0.         0.03429528 0.         0.0302184 ]\n",
      " [0.11751224 0.12421882 0.         0.14517853 0.         0.\n",
      "  0.         0.         0.08092366 0.02081377 0.         0.\n",
      "  0.         0.05057567 1.         0.11825873 0.04025205 0.0309801\n",
      "  0.         0.         0.05491237 0.         0.04838462]\n",
      " [0.06874723 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04734211 0.0303953  0.         0.\n",
      "  0.         0.07385796 0.11825873 1.         0.05878189 0.04524165\n",
      "  0.         0.         0.08019104 0.         0.07065826]\n",
      " [0.02339968 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01611396 0.01034573 0.         0.\n",
      "  0.         0.02513924 0.04025205 0.05878189 1.         0.0614798\n",
      "  0.         0.         0.02729484 0.         0.02405015]\n",
      " [0.03799937 0.         0.         0.         0.01709567 0.01751966\n",
      "  0.04850852 0.         0.01240216 0.05561808 0.         0.02344519\n",
      "  0.0319395  0.01934848 0.0309801  0.04524165 0.0614798  1.\n",
      "  0.10910026 0.03605182 0.02100755 0.         0.01851026]\n",
      " [0.04485464 0.         0.         0.         0.03836069 0.03931208\n",
      "  0.18924327 0.         0.14430459 0.         0.         0.05260828\n",
      "  0.07166851 0.         0.         0.         0.         0.10910026\n",
      "  1.         0.17052207 0.         0.         0.06214546]\n",
      " [0.05478277 0.         0.         0.         0.04685144 0.04801342\n",
      "  0.13293977 0.         0.08359355 0.         0.         0.06425259\n",
      "  0.08753163 0.         0.         0.         0.         0.03605182\n",
      "  0.17052207 1.         0.         0.         0.        ]\n",
      " [0.11043322 0.         0.         0.         0.05683975 0.08372159\n",
      "  0.05376044 0.         0.02198288 0.07288362 0.         0.\n",
      "  0.         0.03429528 0.05491237 0.08019104 0.02729484 0.02100755\n",
      "  0.         0.         1.         0.11639118 0.11350304]\n",
      " [0.         0.         0.         0.         0.07013591 0.\n",
      "  0.         0.         0.         0.         0.         0.09618518\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11639118 1.         0.        ]\n",
      " [0.26564482 0.         0.         0.10554072 0.         0.\n",
      "  0.         0.         0.01936965 0.012436   0.         0.\n",
      "  0.         0.0302184  0.04838462 0.07065826 0.02405015 0.01851026\n",
      "  0.06214546 0.         0.11350304 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return tokens\n",
    "\n",
    "corpus = identify_tokens(df['source'][0])\n",
    "#print(corpus)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=german_stop_words)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "linear_cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial import distance\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return tokens\n",
    "\n",
    "corpus = identify_tokens(df['source'][0])\n",
    "#print(corpus)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "linear_cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(linear_cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentances(cosine_sim,n):\n",
    "    idx = df\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(v, key=lambda x:x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:n+1]\n",
    "    indices = [i[0] for i in sim_scores]\n",
    "    return df1.illoc[indices]\n",
    "\n",
    "print(get_top_sentances(cosine_sim, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize_list = sent_tokenize(df['source'])\n",
    "len_words = len(sent_tokenize_list)\n",
    "print (len_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(numbers):\n",
    "    return sum(numbers)/len(numbers)\n",
    "    sentence = input(\"Please enter a sentence: \")\n",
    "    words = sentence.split()\n",
    "    lengths = [len(word) for word in words]\n",
    "    print 'Average length:', average(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468bc1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/processed.csv', encoding='utf-8')\n",
    "\n",
    "df2 = df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e1ef2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, source, sim_sent, keywords, noCap_LetterWords_inSentence, no_words_inSent_SK, no_words_inSent_SS, sent_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b97dec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Minghella war der Sohn italienisch-schottisch...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>oscar regie opera bbc film bestes fernsehserie...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>theorie film filme autor regisseur ansatz krit...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...</td>\n",
       "      <td>[26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>theatre al new the theaterstu richard yorker i...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...</td>\n",
       "      <td>[8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...</td>\n",
       "      <td>[36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>sungen wasser eigenschaften reaktion lo erfolg...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...</td>\n",
       "      <td>[21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>geregelt ra gewerkschaften bgb betrieben arbei...</td>\n",
       "      <td>[5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...</td>\n",
       "      <td>[10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  [Minghella war der Sohn italienisch-schottisch...   \n",
       "1           1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2           2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3           3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4           4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  oscar regie opera bbc film bestes fernsehserie...   \n",
       "1  theorie film filme autor regisseur ansatz krit...   \n",
       "2  theatre al new the theaterstu richard yorker i...   \n",
       "3  sungen wasser eigenschaften reaktion lo erfolg...   \n",
       "4  geregelt ra gewerkschaften bgb betrieben arbei...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
       "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
       "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
       "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
       "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
       "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
       "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
       "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
       "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
       "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e8e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df2[\"source\"].apply(lambda x : sent_tokenize(x)).apply(pd.Series,1).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61fae16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.DataFrame(df2)\n",
    "#df3['source'] = df3.apply(lambda x: sent_tokenize(x['source'], language=\"german\"),axis=1)\n",
    "\n",
    "df3 = df.explode('source').explode('sim_sent').explode('noCap_LetterWords_inSentence').\\\n",
    "explode('no_words_inSent_SK').explode('no_words_inSent_SS').explode('sent_len').reset_index(drop=True)\n",
    "print(df3['sim_sent'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e670ae1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bfd1ffacf569>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sim_sent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sim_sent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "d = pd.DataFrame(df2['sim_sent'])\n",
    "d= d.explode('sim_sent').reset_index(drop=True)\n",
    "d.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
