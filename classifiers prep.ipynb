{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame({'Text Column':['Sample Text1','Sample Text2'],'Numeric Column': [2, 1]})\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c690f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Text Column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse\n",
    "\n",
    "tv = TfidfVectorizer(min_df = 0.05, max_df = 0.5, stop_words = 'english')\n",
    "X = tv.fit_transform(dataset['Text Column'])\n",
    "vocab = tv.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.DataFrame(X.toarray(), columns = vocab)\n",
    "X1['Numeric Column'] = dataset['Numeric Column']\n",
    "\n",
    "\n",
    "#X_sparse = sparse.csr_matrix(X1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1)\n",
    "#print(X1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48c04d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende 1940er Jahre wurde erste Auteur-Theorie f...</td>\n",
       "      <td>[2, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 2, 2, 1, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 2, 3, 8, 13, 6, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino , geboren Manhattan , Sohn Salvatore...</td>\n",
       "      <td>[4, 0, 2, 1, 2, 1, 0, 2, 4, 0, 1, 1, 1, 2, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, ...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 2, 3, 17, 8, 4, 11, 3, 10, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name Alkalimetalle leitet arabischen Wort ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 5, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 4, 2, 5, 3, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit bereits seit Altertum Gegenstand re...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 5, 5, 8, 3, 3, 6, 8, 6, 7, 8, 6, 4, 8, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1           1  Ende 1940er Jahre wurde erste Auteur-Theorie f...   \n",
       "2           2  Al Pacino , geboren Manhattan , Sohn Salvatore...   \n",
       "3           3  Der Name Alkalimetalle leitet arabischen Wort ...   \n",
       "4           4  Die Arbeit bereits seit Altertum Gegenstand re...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, ...   \n",
       "1  [2, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 2, 2, 1, 3, ...   \n",
       "2  [4, 0, 2, 1, 2, 1, 0, 2, 4, 0, 1, 1, 1, 2, 0, ...   \n",
       "3  [1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 5, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, ...   \n",
       "2  [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...  \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 2, 3, 8, 13, 6, 6...  \n",
       "2  [15, 3, 13, 8, 7, 2, 3, 17, 8, 4, 11, 3, 10, 1...  \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 4, 2, 5, 3, 5, ...  \n",
       "4  [5, 5, 5, 8, 3, 3, 6, 8, 6, 7, 8, 6, 4, 8, 5, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_features.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b541b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = df.head(2)\n",
    "frame1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09211db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3de4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "tv = TfidfVectorizer(min_df = 2, max_df = 5, stop_words = german_stop_words)\n",
    "X = tv.fit_transform(frame1['source'])\n",
    "vocab = tv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.DataFrame(X.toarray(), columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102344f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1['noCap_LetterWords_inSentence'] = numpy.array(frame1['noCap_LetterWords_inSentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a640df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8dc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(df):\n",
    "    return sent_tokenize(df, language=\"german\")\n",
    "df2 = pd.concat([pd.DataFrame({'source': source}, index=[0]) \n",
    "           for _, row in df.iterrows() \n",
    "           for source in row['source'].split('.') if source != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4536afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e07399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "def sent(df):\n",
    "    return sent_tokenize(df, language=\"german\")\n",
    "df.source = df.source.apply(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([pd.DataFrame({'source': source, 'no_words_inSent_SK': row['no_words_inSent_SK']}, index=[0]) \n",
    "           for _, row in df.iterrows() \n",
    "           for source in row['source'].split('.') if source != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1835ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06db9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([pd.Series(row['sim_sent'].split(','))              \n",
    "                    for s, row in df.iterrows() if s != '[' or ']' or ' ' or ',']).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8066db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(df):\n",
    "    for s in df:\n",
    "        if s == '[' or s == ']':\n",
    "            return ''\n",
    "        else:\n",
    "            return s\n",
    "df3=df3.apply(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f646895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 18310\n",
    "df3 = df3.head(df3.shape[0] -N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a242b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           \n",
       "1           \n",
       "2           \n",
       "3           \n",
       "4           \n",
       "          ..\n",
       "3513930     \n",
       "3513931     \n",
       "3513932     \n",
       "3513933     \n",
       "3513934     \n",
       "Length: 3513935, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ed5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/class_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a95dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, treetaggerwrapper, networkx as nx, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk, string\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "nltk.download('wordnet')\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a51a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_features.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669e9313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende 1940er Jahre wurde erste Auteur-Theorie f...</td>\n",
       "      <td>[2, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 2, 2, 1, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 2, 3, 8, 13, 6, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino , geboren Manhattan , Sohn Salvatore...</td>\n",
       "      <td>[4, 0, 2, 1, 2, 1, 0, 2, 4, 0, 1, 1, 1, 2, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, ...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 2, 3, 17, 8, 4, 11, 3, 10, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name Alkalimetalle leitet arabischen Wort ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 5, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 4, 2, 5, 3, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit bereits seit Altertum Gegenstand re...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 5, 5, 8, 3, 3, 6, 8, 6, 7, 8, 6, 4, 8, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1           1  Ende 1940er Jahre wurde erste Auteur-Theorie f...   \n",
       "2           2  Al Pacino , geboren Manhattan , Sohn Salvatore...   \n",
       "3           3  Der Name Alkalimetalle leitet arabischen Wort ...   \n",
       "4           4  Die Arbeit bereits seit Altertum Gegenstand re...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, ...   \n",
       "1  [2, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 2, 2, 1, 3, ...   \n",
       "2  [4, 0, 2, 1, 2, 1, 0, 2, 4, 0, 1, 1, 1, 2, 0, ...   \n",
       "3  [1, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 5, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, ...   \n",
       "2  [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...  \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 2, 3, 8, 13, 6, 6...  \n",
       "2  [15, 3, 13, 8, 7, 2, 3, 17, 8, 4, 11, 3, 10, 1...  \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 4, 2, 5, 3, 5, ...  \n",
       "4  [5, 5, 5, 8, 3, 3, 6, 8, 6, 7, 8, 6, 4, 8, 5, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Getting s as pandas series which has split on full stop and new sentence a new line         \n",
    "s = df[\"source\"].apply(lambda x : sent_tokenize(x)).apply(pd.Series,1).stack()\n",
    "s.index = s.index.droplevel(-1) # to line up with df's index\n",
    "s.name = 'source' # needs a name to join\n",
    "\n",
    "x=s.to_frame(name='source1')\n",
    "\n",
    "# Joining should ideally get me proper output. But I am getting original dataframe instead of split one.\n",
    "df2 = df.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e4f92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "   return sent_tokenize(row, language='german')\n",
    "\n",
    "df['source'] = df['source'].apply(identify_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cfd3d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 4588, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[1;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         codes, uniques = factorize_array(\n\u001b[0m\u001b[0;32m    723\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize_array\u001b[1;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     uniques, codes = table.factorize(\n\u001b[0m\u001b[0;32m    529\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6808160c71fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m (df.set_index('source')\n\u001b[0m\u001b[0;32m      2\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    .drop('level_1', 1))\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7766\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7767\u001b[0m         )\n\u001b[1;32m-> 7768\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-6808160c71fc>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m (df.set_index('source')\n\u001b[1;32m----> 2\u001b[1;33m    \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    .drop('level_1', 1))\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(self, level, dropna)\u001b[0m\n\u001b[0;32m   7208\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack_multiple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7209\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7210\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"stack\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(frame, level, dropna)\u001b[0m\n\u001b[0;32m    513\u001b[0m         )\n\u001b[0;32m    514\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0milab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclab\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0milab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         new_index = MultiIndex(\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mfactorize\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize_from_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterable\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   2602\u001b[0m         \u001b[1;31m# but only the resulting categories, the order of which is independent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m         \u001b[1;31m# from ordered. Set ordered to False as default. See GH #15457\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2604\u001b[1;33m         \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2605\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2606\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \u001b[1;31m# raise, as we don't have a sortable data structure and so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[1;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mna_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         codes, uniques = factorize_array(\n\u001b[0m\u001b[0;32m    723\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         )\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mfactorize_array\u001b[1;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhash_klass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_hint\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     uniques, codes = table.factorize(\n\u001b[0m\u001b[0;32m    529\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     )\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "(df.set_index('source')\n",
    "   .apply(lambda x: x.apply(pd.Series).stack())\n",
    "   .reset_index()\n",
    "   .drop('level_1', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapWords(df):\n",
    "    return sum(1 for c in df if c.isupper())\n",
    "\n",
    "df_new['noCap_LetterWords_inSentence'] = df_new['source'].apply(CapWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5db016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
