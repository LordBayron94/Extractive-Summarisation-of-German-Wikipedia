{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5290a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "import os, treetaggerwrapper, networkx as nx, collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796d4e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>oscar regie opera bbc film bestes fernsehserie...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>theorie film filme autor regisseur ansatz krit...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...</td>\n",
       "      <td>[26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>theatre al new the theaterstu richard yorker i...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...</td>\n",
       "      <td>[8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...</td>\n",
       "      <td>[36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>sungen wasser eigenschaften reaktion lo erfolg...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...</td>\n",
       "      <td>[21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>geregelt ra gewerkschaften bgb betrieben arbei...</td>\n",
       "      <td>[5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...</td>\n",
       "      <td>[10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella war der Sohn italienisch-schottische...   \n",
       "1           1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2           2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3           3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4           4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  oscar regie opera bbc film bestes fernsehserie...   \n",
       "1  theorie film filme autor regisseur ansatz krit...   \n",
       "2  theatre al new the theaterstu richard yorker i...   \n",
       "3  sungen wasser eigenschaften reaktion lo erfolg...   \n",
       "4  geregelt ra gewerkschaften bgb betrieben arbei...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
       "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
       "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
       "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
       "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
       "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
       "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
       "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
       "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
       "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/processed.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05da820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.         0.02597783 0.02662211\n",
      "  0.07371142 0.         0.01884577 0.01209965 0.         0.03562628\n",
      "  0.04853386 0.02940111 0.11751224 0.06874723 0.02339968 0.03799937\n",
      "  0.04485464 0.05478277 0.11043322 0.         0.26564482]\n",
      " [0.         1.         0.         0.1810938  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.12421882 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.1810938  0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14517853 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.10554072]\n",
      " [0.02597783 0.         0.         0.         1.         0.14553316\n",
      "  0.10245552 0.         0.         0.04308864 0.         0.09798114\n",
      "  0.04150724 0.         0.         0.         0.         0.01709567\n",
      "  0.03836069 0.04685144 0.05683975 0.07013591 0.        ]\n",
      " [0.02662211 0.         0.         0.         0.14553316 1.\n",
      "  0.12266036 0.         0.         0.         0.         0.03122405\n",
      "  0.04253667 0.         0.         0.         0.         0.01751966\n",
      "  0.03931208 0.04801342 0.08372159 0.         0.        ]\n",
      " [0.07371142 0.         0.         0.         0.10245552 0.12266036\n",
      "  1.         0.         0.03749232 0.0407543  0.         0.08645327\n",
      "  0.11777572 0.         0.         0.         0.         0.04850852\n",
      "  0.18924327 0.13293977 0.05376044 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.01884577 0.         0.         0.         0.         0.\n",
      "  0.03749232 0.         1.         0.00833231 0.         0.\n",
      "  0.         0.02024679 0.08092366 0.04734211 0.01611396 0.01240216\n",
      "  0.14430459 0.08359355 0.02198288 0.         0.01936965]\n",
      " [0.01209965 0.         0.         0.         0.04308864 0.\n",
      "  0.0407543  0.         0.00833231 1.         0.034051   0.\n",
      "  0.02892613 0.01299915 0.02081377 0.0303953  0.01034573 0.05561808\n",
      "  0.         0.         0.07288362 0.         0.012436  ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.034051   1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.03562628 0.         0.         0.         0.09798114 0.03122405\n",
      "  0.08645327 0.         0.         0.         0.         1.\n",
      "  0.05692349 0.         0.         0.         0.         0.02344519\n",
      "  0.05260828 0.06425259 0.         0.09618518 0.        ]\n",
      " [0.04853386 0.         0.         0.         0.04150724 0.04253667\n",
      "  0.11777572 0.         0.         0.02892613 0.         0.05692349\n",
      "  1.         0.         0.         0.         0.         0.0319395\n",
      "  0.07166851 0.08753163 0.         0.         0.        ]\n",
      " [0.02940111 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.02024679 0.01299915 0.         0.\n",
      "  0.         1.         0.05057567 0.07385796 0.02513924 0.01934848\n",
      "  0.         0.         0.03429528 0.         0.0302184 ]\n",
      " [0.11751224 0.12421882 0.         0.14517853 0.         0.\n",
      "  0.         0.         0.08092366 0.02081377 0.         0.\n",
      "  0.         0.05057567 1.         0.11825873 0.04025205 0.0309801\n",
      "  0.         0.         0.05491237 0.         0.04838462]\n",
      " [0.06874723 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04734211 0.0303953  0.         0.\n",
      "  0.         0.07385796 0.11825873 1.         0.05878189 0.04524165\n",
      "  0.         0.         0.08019104 0.         0.07065826]\n",
      " [0.02339968 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01611396 0.01034573 0.         0.\n",
      "  0.         0.02513924 0.04025205 0.05878189 1.         0.0614798\n",
      "  0.         0.         0.02729484 0.         0.02405015]\n",
      " [0.03799937 0.         0.         0.         0.01709567 0.01751966\n",
      "  0.04850852 0.         0.01240216 0.05561808 0.         0.02344519\n",
      "  0.0319395  0.01934848 0.0309801  0.04524165 0.0614798  1.\n",
      "  0.10910026 0.03605182 0.02100755 0.         0.01851026]\n",
      " [0.04485464 0.         0.         0.         0.03836069 0.03931208\n",
      "  0.18924327 0.         0.14430459 0.         0.         0.05260828\n",
      "  0.07166851 0.         0.         0.         0.         0.10910026\n",
      "  1.         0.17052207 0.         0.         0.06214546]\n",
      " [0.05478277 0.         0.         0.         0.04685144 0.04801342\n",
      "  0.13293977 0.         0.08359355 0.         0.         0.06425259\n",
      "  0.08753163 0.         0.         0.         0.         0.03605182\n",
      "  0.17052207 1.         0.         0.         0.        ]\n",
      " [0.11043322 0.         0.         0.         0.05683975 0.08372159\n",
      "  0.05376044 0.         0.02198288 0.07288362 0.         0.\n",
      "  0.         0.03429528 0.05491237 0.08019104 0.02729484 0.02100755\n",
      "  0.         0.         1.         0.11639118 0.11350304]\n",
      " [0.         0.         0.         0.         0.07013591 0.\n",
      "  0.         0.         0.         0.         0.         0.09618518\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11639118 1.         0.        ]\n",
      " [0.26564482 0.         0.         0.10554072 0.         0.\n",
      "  0.         0.         0.01936965 0.012436   0.         0.\n",
      "  0.         0.0302184  0.04838462 0.07065826 0.02405015 0.01851026\n",
      "  0.06214546 0.         0.11350304 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return tokens\n",
    "\n",
    "corpus = identify_tokens(df['source'][0])\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=german_stop_words)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "linear_cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882fa333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, string, numpy\n",
    "from nltk.corpus import stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "nltk.download('punkt') # first-time use only\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def StemTokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def StemNormalize(text):\n",
    "    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "nltk.download('wordnet') # first-time use only\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=german_stop_words)\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca617cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    for tup in output_tuples:\\n        for sent in tup:\\n            similar_sent.append(sent)\\n    for sent in doc:\\n        if sent in set(doc).intersection(set(similar_sent)):\\n            tags.append(1)\\n        else:\\n            tags.append(0)\\n    return tags'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_docs(df):\n",
    "    threshold = 0.15\n",
    "    doc = sent_tokenize(df, language=\"german\")\n",
    "    lst, tags, output_tuples, similar_sent = [], [], [], [] \n",
    "    arr = cos_similarity(doc)\n",
    "    doc_names = [\"s\"+str(s) for s in range(0, len(doc), 1)]\n",
    "    for row in range(len(arr)):\n",
    "        lst = [row+1+idx for idx, num in \\\n",
    "                  enumerate(arr[row, row+1:]) if num >= threshold]\n",
    "        [(output_tuples.append((doc[row], doc[item]))) for item in lst]\n",
    "        print(arr)\n",
    "'''    for tup in output_tuples:\n",
    "        for sent in tup:\n",
    "            similar_sent.append(sent)\n",
    "    for sent in doc:\n",
    "        if sent in set(doc).intersection(set(similar_sent)):\n",
    "            tags.append(1)\n",
    "        else:\n",
    "            tags.append(0)\n",
    "    return tags'''\n",
    "#get_docs(df['source'][0])\n",
    "#df['sim_sent'] = df['source'].apply(get_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456e575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>oscar regie opera bbc film bestes fernsehserie...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>theorie film filme autor regisseur ansatz krit...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...</td>\n",
       "      <td>[26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>theatre al new the theaterstu richard yorker i...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...</td>\n",
       "      <td>[8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...</td>\n",
       "      <td>[36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>sungen wasser eigenschaften reaktion lo erfolg...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...</td>\n",
       "      <td>[21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>geregelt ra gewerkschaften bgb betrieben arbei...</td>\n",
       "      <td>[5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...</td>\n",
       "      <td>[10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Mit \"Ampelkoalition\" wird in Deutschland übli...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>koalition fdp nen gru spd liberalen ndnis rot ...</td>\n",
       "      <td>[18, 7, 7, 8, 3, 4, 5, 6, 19, 28, 7, 16, 22, 1...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7, 1, 6, 3, 2, 4, 2, 1, 4, 6, 3, 3, 3, 2, 2, ...</td>\n",
       "      <td>[29, 21, 26, 24, 10, 8, 12, 15, 36, 52, 17, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Im Mittelalter trat Ergotismus als Folge des V...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>antonius getreide feuer krankheiten sekunda an...</td>\n",
       "      <td>[10, 6, 7, 10, 3, 10, 3, 4, 9, 9, 10, 4, 3, 6,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3, 2, 5, 1, 1, 5, 2, 1, 4, 2, 4, 1, 3, 1, 0, ...</td>\n",
       "      <td>[26, 20, 20, 22, 8, 25, 12, 9, 20, 14, 22, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Arnims Vater war der wohlhabende Königlich Pr...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>goethe erza weimar go grimm berlin berliner sc...</td>\n",
       "      <td>[18, 5, 3, 13, 5, 9, 9, 8, 4, 9, 6, 5, 8, 7, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4, 1, 0, 4, 2, 3, 1, 6, 2, 1, 2, 3, 3, 2, 3, ...</td>\n",
       "      <td>[36, 7, 7, 32, 14, 24, 17, 18, 14, 17, 17, 17,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Es gibt drei klassische Aggregatzustände: Fü...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, ...</td>\n",
       "      <td>temperatur flu gas phase bewegung ssigkeit sto...</td>\n",
       "      <td>[7, 9, 6, 4, 5, 5, 4, 4, 4, 4, 2, 7, 3, 3, 10,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6, 10, 4, 6, 7, 4, 1, 3, 1, 5, 0, 2, 1, 0, 3,...</td>\n",
       "      <td>[18, 28, 15, 12, 19, 14, 9, 11, 14, 19, 2, 19,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Börsenkurse sind dadurch gekennzeichnet, dass...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>bo handels handel rse kurs nnen ftsfu aktien d...</td>\n",
       "      <td>[2, 5, 7, 3, 4, 13, 9, 5, 7, 8, 7, 3, 15, 6, 7...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 3, 4, 0, 1, 8, 5, 1, 1, 3, 6, 2, 6, 1, 7, ...</td>\n",
       "      <td>[12, 16, 28, 7, 7, 34, 34, 13, 16, 18, 25, 15,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella war der Sohn italienisch-schottische...   \n",
       "1           1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2           2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3           3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4           4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "5           5  Mit \"Ampelkoalition\" wird in Deutschland übli...   \n",
       "6           6  Im Mittelalter trat Ergotismus als Folge des V...   \n",
       "7           7  Arnims Vater war der wohlhabende Königlich Pr...   \n",
       "8           8  Es gibt drei klassische Aggregatzustände: Fü...   \n",
       "9           9  Börsenkurse sind dadurch gekennzeichnet, dass...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...   \n",
       "2  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5  [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, ...   \n",
       "6  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "7  [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, ...   \n",
       "8  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, ...   \n",
       "9  [0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  oscar regie opera bbc film bestes fernsehserie...   \n",
       "1  theorie film filme autor regisseur ansatz krit...   \n",
       "2  theatre al new the theaterstu richard yorker i...   \n",
       "3  sungen wasser eigenschaften reaktion lo erfolg...   \n",
       "4  geregelt ra gewerkschaften bgb betrieben arbei...   \n",
       "5  koalition fdp nen gru spd liberalen ndnis rot ...   \n",
       "6  antonius getreide feuer krankheiten sekunda an...   \n",
       "7  goethe erza weimar go grimm berlin berliner sc...   \n",
       "8  temperatur flu gas phase bewegung ssigkeit sto...   \n",
       "9  bo handels handel rse kurs nnen ftsfu aktien d...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
       "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
       "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
       "5  [18, 7, 7, 8, 3, 4, 5, 6, 19, 28, 7, 16, 22, 1...   \n",
       "6  [10, 6, 7, 10, 3, 10, 3, 4, 9, 9, 10, 4, 3, 6,...   \n",
       "7  [18, 5, 3, 13, 5, 9, 9, 8, 4, 9, 6, 5, 8, 7, 1...   \n",
       "8  [7, 9, 6, 4, 5, 5, 4, 4, 4, 4, 2, 7, 3, 3, 10,...   \n",
       "9  [2, 5, 7, 3, 4, 13, 9, 5, 7, 8, 7, 3, 15, 6, 7...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5  [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7  [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9  [1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
       "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
       "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
       "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
       "5  [7, 1, 6, 3, 2, 4, 2, 1, 4, 6, 3, 3, 3, 2, 2, ...   \n",
       "6  [3, 2, 5, 1, 1, 5, 2, 1, 4, 2, 4, 1, 3, 1, 0, ...   \n",
       "7  [4, 1, 0, 4, 2, 3, 1, 6, 2, 1, 2, 3, 3, 2, 3, ...   \n",
       "8  [6, 10, 4, 6, 7, 4, 1, 3, 1, 5, 0, 2, 1, 0, 3,...   \n",
       "9  [1, 3, 4, 0, 1, 8, 5, 1, 1, 3, 6, 2, 6, 1, 7, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
       "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
       "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
       "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
       "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  \n",
       "5  [29, 21, 26, 24, 10, 8, 12, 15, 36, 52, 17, 30...  \n",
       "6  [26, 20, 20, 22, 8, 25, 12, 9, 20, 14, 22, 8, ...  \n",
       "7  [36, 7, 7, 32, 14, 24, 17, 18, 14, 17, 17, 17,...  \n",
       "8  [18, 28, 15, 12, 19, 14, 9, 11, 14, 19, 2, 19,...  \n",
       "9  [12, 16, 28, 7, 7, 34, 34, 13, 16, 18, 25, 15,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21e5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re, nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85,stop_words=german_stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(df['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91b5423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74567863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_\n",
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca35b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4e76f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyw(df):\n",
    "    kljuc = []\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([df]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    for k in keywords:\n",
    "        kljuc.append(k)\n",
    "    return kljuc\n",
    "\n",
    "df['keywords'] = df['source'].apply(keyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040230e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[oscar, regie, opera, bbc, film, bestes, ferns...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[theorie, film, filme, autor, regisseur, ansat...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...</td>\n",
       "      <td>[26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[theatre, al, new, the, theaterstu, richard, y...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...</td>\n",
       "      <td>[8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...</td>\n",
       "      <td>[36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sungen, wasser, eigenschaften, reaktion, lo, ...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...</td>\n",
       "      <td>[21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[geregelt, ra, gewerkschaften, bgb, betrieben,...</td>\n",
       "      <td>[5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...</td>\n",
       "      <td>[10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella war der Sohn italienisch-schottische...   \n",
       "1           1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2           2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3           3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4           4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...   \n",
       "2  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [oscar, regie, opera, bbc, film, bestes, ferns...   \n",
       "1  [theorie, film, filme, autor, regisseur, ansat...   \n",
       "2  [theatre, al, new, the, theaterstu, richard, y...   \n",
       "3  [sungen, wasser, eigenschaften, reaktion, lo, ...   \n",
       "4  [geregelt, ra, gewerkschaften, bgb, betrieben,...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
       "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
       "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
       "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
       "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
       "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
       "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
       "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
       "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
       "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9832156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[oscar, regie, opera, bbc, film, bestes, ferns...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[theorie, film, filme, autor, regisseur, ansat...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...</td>\n",
       "      <td>[26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[theatre, al, new, the, theaterstu, richard, y...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...</td>\n",
       "      <td>[8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...</td>\n",
       "      <td>[36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sungen, wasser, eigenschaften, reaktion, lo, ...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...</td>\n",
       "      <td>[21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[geregelt, ra, gewerkschaften, bgb, betrieben,...</td>\n",
       "      <td>[5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...</td>\n",
       "      <td>[10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella war der Sohn italienisch-schottische...   \n",
       "1           1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2           2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3           3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4           4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...   \n",
       "2  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [oscar, regie, opera, bbc, film, bestes, ferns...   \n",
       "1  [theorie, film, filme, autor, regisseur, ansat...   \n",
       "2  [theatre, al, new, the, theaterstu, richard, y...   \n",
       "3  [sungen, wasser, eigenschaften, reaktion, lo, ...   \n",
       "4  [geregelt, ra, gewerkschaften, bgb, betrieben,...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
       "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
       "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
       "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
       "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
       "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
       "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
       "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
       "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
       "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def capital_lWords(df):\n",
    "    tokens = sent_tokenize(df, language=\"german\")\n",
    "    return [sum([c.isupper() for c in a]) for a in tokens]\n",
    "\n",
    "df['noCap_LetterWords_inSentence'] = df['source'].apply(capital_lWords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e79c5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_sourceSummary(df, df2):\n",
    "    f = []\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len(list((set(s.split()).intersection(set(tok2))))))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5de0c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_sourceKeywords(df, df2):\n",
    "    f = []\n",
    "    df.lower()\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len(list((set(s.split()).intersection(set(tok2))))))\n",
    "    return f\n",
    "\n",
    "def listToString(df):\n",
    "    string = ' '.join([str(e) for e in df])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9f206f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'] = df['keywords'].apply(listToString)\n",
    "df['no_words_inSent_SK'] = df.apply(lambda x: W_sourceKeywords(x['source'], x['keywords']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4eb48daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_words_inSent_SS'] = df.apply(lambda x: W_sourceSummary(x['source'], x['summary']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79856e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Minghella', 'war', 'der', 'Sohn', 'italienisch-schottischer', 'Eltern,', 'die', 'auf', 'der', 'Isle', 'of', 'Wight', 'eine', 'Fabrik', 'für', 'Eiscreme', 'betrieben.'], ['Nach', 'seinem', 'Schulabschluss', 'studierte', 'er', 'an', 'der', 'Universität', 'Hull,', 'wo', 'er', 'eine', 'Zeit', 'lang', 'als', 'Dozent', 'tätig', 'war.'], ['1978', 'drehte', 'er', 'einen', 'ersten', 'Kurzfilm.'], ['Seit', '1981', 'war', 'er', 'als', 'Autor', 'und', 'Story', 'Editor', 'tätig.'], ['Er', 'wurde', 'mit', 'Theaterstücken,', 'Rundfunkhörspielen,', 'der', 'Fernsehserie', '\"Inspector', 'Morse\"', 'und', 'vielen', 'Drehbüchern', 'für', 'Film', 'und', 'Fernsehen', 'bekannt.'], ['Er', 'entwickelte', 'die', 'Drehbücher', 'für', 'die', '1988', 'erfolgreich', 'ausgestrahlte', 'Fernsehserie', 'The', 'Storyteller', 'von', 'Muppets-Erfinder', 'Jim', 'Henson.'], ['Auch', 'als', 'Produzent', 'war', 'er', 'erfolgreich,', 'darunter', 'für', 'die', 'Filme', '\"Der', 'stille', 'Amerikaner\",', '\"Die', 'Dolmetscherin\"', 'und', '\"Der', 'Vorleser\",', 'für', 'den', 'er', '2008', 'posthum', 'für', 'den', 'Oscar', 'nominiert', 'wurde.'], ['Gemeinsam', 'mit', 'seinem', 'Freund', 'und', 'Kollegen', 'Sydney', 'Pollack', 'gründete', 'er', 'die', 'Produktionsfirma', 'Mirage', 'Enterprises.'], ['Der', 'Regisseur', 'Minghella', 'galt', 'als', 'ein', 'guter', 'Schauspielerführer:', 'Unter', 'seiner', 'Regie', 'brachten', 'es', 'zahlreiche', 'Darsteller', 'zu', 'Oscar-Nominierungen,', 'zwei', 'Schauspielerinnen', 'erhielten', 'die', 'Auszeichnung', 'als', '\"Beste', 'Nebendarstellerin\":', 'Juliette', 'Binoche', 'und', 'Renée', 'Zellweger', '.'], ['Gegen', 'Ende', 'seines', 'Lebens', 'kehrte', 'Minghella', 'zu', 'seinen', 'Anfängen', 'im', 'Radio', 'und', 'auf', 'der', 'Bühne', 'zurück:', '2006', 'wurde', 'sein', 'Hörspiel', '\"Eyes', 'Down', 'Looking\"', 'mit', 'Jude', 'Law', 'zu', 'Ehren', 'von', 'Samuel', 'Beckett', 'auf', 'BBC', 'Radio', '3', 'ausgestrahlt,', 'ein', 'Jahr', 'zuvor', 'hatte', 'seine', 'Inszenierung', 'der', 'Puccini-Oper', 'Madame', 'Butterfly', 'in', 'der', 'English', 'National', 'Opera', 'in', 'London', 'Premiere', 'und', 'wurde', 'auch', 'in', 'der', 'Nationaloper', 'von', 'Vilnius', 'und', 'in', 'der', 'Metropolitan', 'Opera', 'in', 'New', 'York', 'gezeigt.'], ['Am', 'Ende', 'des', 'Films', '\"Abbitte\"', 'von', 'Joe', 'Wright', 'hat', 'er', 'einen', 'Kurzauftritt', 'als', 'Talkshow-Moderator', 'neben', 'Vanessa', 'Redgrave.'], ['Seine', 'letzte', 'Arbeit', 'als', 'Drehbuchautor', 'war', 'das', 'Skript', 'für', 'den', 'Musical-Film', '\"Nine\"', '.'], ['Zu', 'seinen', 'letzten', 'Regiearbeiten', 'zählt', 'der', 'Pilotfilm', 'zur', 'Krimiserie', '\"Eine', 'Detektivin', 'für', 'Botswana\"', ',', 'den', 'die', 'BBC', 'fünf', 'Tage', 'nach', 'seinem', 'Tod', 'erstmals', 'ausstrahlte.'], ['Minghella', 'war', 'mit', 'der', 'aus', 'Hongkong', 'stammenden', 'Choreographin,', 'Produzentin', 'und', 'Schauspielerin', 'Carolyn', 'Choa', 'verheiratet.'], ['Der', 'Ehe', 'entstammen', 'zwei', 'Kinder,', 'die', 'in', 'der', 'Filmbranche', 'tätig', 'sind:', 'Tochter', 'Hannah', 'Minghella', 'in', 'der', 'Produktion', 'und', 'Sohn', 'Max', 'Minghella', 'als', 'Schauspieler', '.'], ['Die', 'Tante', 'Edana', 'Minghella', 'und', 'der', 'Onkel', 'Dominic', 'Minghella', 'sind', 'Drehbuchautoren.'], ['Minghella', 'starb', 'im', 'Alter', 'von', '54', 'Jahren', 'in', 'einem', 'Londoner', 'Krankenhaus', 'an', 'inneren', 'Blutungen', 'infolge', 'der', 'Operation', 'eines', 'Tonsillenkarzinoms', 'und', 'eines', 'Karzinoms', 'im', 'Nacken.'], ['1984', 'erhielt', 'Minghella', 'den', 'Londoner', 'Kritikerpreis', 'als', 'meistversprechender', 'junger', 'Dramatiker,', '1986', 'den', 'Kritikerpreis', 'für', 'sein', 'Stück', '\"Made', 'in', 'Bangkok\"', 'als', 'bestes', 'Stück', 'der', 'Saison.'], ['1997', 'erhielt', 'er', 'für', '\"Der', 'englische', 'Patient\"', 'den', 'Oscar', 'in', 'der', 'Rubrik', '\"Beste', 'Regie\",', '1999', 'eine', 'Oscar-Nominierung', 'in', 'der', 'Kategorie', '\"Bestes', 'adaptiertes', 'Drehbuch\"', 'für', '\"Der', 'talentierte', 'Mr.'], ['Ripley\",', 'bei', 'dem', 'er', 'auch', 'Regie', 'führte.'], ['2001', 'wurde', 'Minghella', 'zum', 'Commander', 'of', 'the', 'British', 'Empire', 'ernannt.'], ['Von', '2003', 'bis', '2007', 'war', 'er', 'Präsident', 'des', 'British', 'Film', 'Institute.'], ['Seit', '1997', 'trägt', 'das', 'Anthony', 'Minghella', 'Theatre', 'auf', 'der', 'Isle', 'of', 'Wight', 'seinen', 'Namen.']]\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "tokens = sent_tokenize(df['source'][0], language='german')\n",
    "for s in tokens:\n",
    "    words = s.split()\n",
    "    lst.append(words)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b702503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_len(row):\n",
    "    lst  = []\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    [(lst.append(len(s.split()))) for s in tokens]\n",
    "    return lst\n",
    "df['sent_len'] = df['source'].apply(s_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82026dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>Anthony Minghella, CBE war ein britischer Film...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>oscar regie opera bbc film bestes fernsehserie...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>Die Auteur-Theorie ist eine Filmtheorie und di...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>theorie film filme autor regisseur ansatz krit...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...</td>\n",
       "      <td>[26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>Alfredo James \"Al\" Pacino ist ein US-amerikani...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>theatre al new the theaterstu richard yorker i...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...</td>\n",
       "      <td>[8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...</td>\n",
       "      <td>[36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>Als Alkalimetalle werden die chemischen Elemen...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>sungen wasser eigenschaften reaktion lo erfolg...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...</td>\n",
       "      <td>[21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>Das deutsche Arbeitsrecht ist ein Rechtsgebiet...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>geregelt ra gewerkschaften bgb betrieben arbei...</td>\n",
       "      <td>[5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...</td>\n",
       "      <td>[10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Minghella war der Sohn italienisch-schottische...   \n",
       "1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Anthony Minghella, CBE war ein britischer Film...   \n",
       "1  Die Auteur-Theorie ist eine Filmtheorie und di...   \n",
       "2  Alfredo James \"Al\" Pacino ist ein US-amerikani...   \n",
       "3  Als Alkalimetalle werden die chemischen Elemen...   \n",
       "4  Das deutsche Arbeitsrecht ist ein Rechtsgebiet...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, ...   \n",
       "2  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  oscar regie opera bbc film bestes fernsehserie...   \n",
       "1  theorie film filme autor regisseur ansatz krit...   \n",
       "2  theatre al new the theaterstu richard yorker i...   \n",
       "3  sungen wasser eigenschaften reaktion lo erfolg...   \n",
       "4  geregelt ra gewerkschaften bgb betrieben arbei...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
       "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
       "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
       "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
       "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
       "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
       "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
       "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
       "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
       "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c85d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = df.apply(lambda x: sent_tokenize(x['source'], language=\"german\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "216a058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              source  \\\n",
      "0  [Minghella war der Sohn italienisch-schottisch...   \n",
      "1  [Ende der 1940er Jahre wurde eine erste Auteur...   \n",
      "2  [Al Pacino, geboren in Manhattan, ist der Sohn...   \n",
      "3  [Der Name der Alkalimetalle leitet sich von de...   \n",
      "4  [Die Arbeit ist bereits seit dem Altertum Gege...   \n",
      "\n",
      "                                            sim_sent  \\\n",
      "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                            keywords  \\\n",
      "0  oscar regie opera bbc film bestes fernsehserie...   \n",
      "1  theorie film filme autor regisseur ansatz krit...   \n",
      "2  theatre al new the theaterstu richard yorker i...   \n",
      "3  sungen wasser eigenschaften reaktion lo erfolg...   \n",
      "4  geregelt ra gewerkschaften bgb betrieben arbei...   \n",
      "\n",
      "                        noCap_LetterWords_inSentence  \\\n",
      "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
      "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 5, 8, 13, 6, 6, 2...   \n",
      "2  [15, 3, 13, 8, 7, 5, 17, 8, 4, 11, 3, 10, 11, ...   \n",
      "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 6, 5, 3, 5, 7, ...   \n",
      "4  [5, 5, 5, 11, 9, 8, 13, 8, 6, 4, 8, 5, 4, 10, ...   \n",
      "\n",
      "                                  no_words_inSent_SK  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                  no_words_inSent_SS  \\\n",
      "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
      "1  [5, 4, 3, 3, 1, 1, 4, 6, 5, 2, 4, 5, 5, 7, 3, ...   \n",
      "2  [8, 2, 6, 4, 2, 3, 8, 6, 4, 2, 3, 4, 5, 3, 11,...   \n",
      "3  [4, 2, 2, 0, 2, 2, 2, 1, 2, 3, 4, 8, 3, 2, 4, ...   \n",
      "4  [2, 1, 0, 2, 1, 6, 2, 3, 3, 3, 3, 3, 1, 4, 4, ...   \n",
      "\n",
      "                                            sent_len  \n",
      "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  \n",
      "1  [26, 17, 16, 13, 9, 8, 19, 15, 25, 19, 26, 36,...  \n",
      "2  [36, 11, 33, 16, 20, 12, 33, 18, 10, 21, 9, 19...  \n",
      "3  [21, 16, 5, 9, 13, 8, 6, 5, 18, 11, 16, 18, 9,...  \n",
      "4  [10, 16, 9, 34, 18, 21, 35, 16, 18, 13, 22, 14...  \n"
     ]
    }
   ],
   "source": [
    "print (df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a27d3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stop_words = stopwords.words('german')\n",
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=german_stop_words, max_df=0.8)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(tokens).toarray()\n",
    "    return tfidf_matrix\n",
    "\n",
    "#df['source'] = df['source'].apply(identify_tokens)\n",
    "\n",
    "#print(tfidf_matrix)\n",
    "\n",
    "df2 = df.head(1)\n",
    "#corpus = sent_tokenize(df2['source'][0], language=\"german\")\n",
    "#tfidf_vectorizer = TfidfVectorizer(stop_words=german_stop_words, min_df=0.1, max_df=0.8)\n",
    "#tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "#print(tfidf_matrix)\n",
    "df['source'] = df['source'].apply(identify_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0954f030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>sim_sent</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>oscar regie opera bbc film bestes fernsehserie...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...</td>\n",
       "      <td>[17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                            sim_sent  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  oscar regie opera bbc film bestes fernsehserie...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \\\n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [2, 0, 0, 2, 1, 0, 2, 1, 3, 3, 0, 2, 0, 3, 2, ...   \n",
       "\n",
       "                                            sent_len  \n",
       "0  [17, 18, 6, 10, 17, 16, 28, 14, 31, 71, 17, 13...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3ece99b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-433d7d064dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Remove all the special characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# remove all single characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['source', 'noCap_LetterWords_inSentence', 'no_words_inSent_SK', 'no_words_inSent_SS', 'sent_len']]\n",
    "y = df['sim_sent']\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66ce835f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e35af949252f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             )\n\u001b[1;32m--> 304\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    305\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
