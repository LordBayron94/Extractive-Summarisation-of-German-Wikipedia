{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97caf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser #We're choosing a plaintext parser here, other parsers available for HTML etc.\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer #We're choosing Lexrank, other algorithms are also built in\n",
    "import pandas as pd, nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9636d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in nltk.word_tokenize(text) if word not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "def lexRank(df, size):\n",
    "    lst = []\n",
    "    parser = PlaintextParser.from_string(df, Tokenizer(\"german\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, size)\n",
    "    for s in summarizer(parser.document, size):\n",
    "        lst.append(s)\n",
    "    string = ' '.join([str(e) for e in lst])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8344f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.head(50)\n",
    "print(lexRank(test_df['source'][0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['source'] = test_df['source'].apply(remove_stopwords)\n",
    "test_df['lex1'] = test_df.apply(lambda x: lexRank(x['source'], 1), axis=1)\n",
    "test_df['lex2'] = test_df.apply(lambda x: lexRank(x['source'], 2), axis=1)\n",
    "test_df['lex5'] = test_df.apply(lambda x: lexRank(x['source'], 5), axis=1)\n",
    "test_df['lex10'] = test_df.apply(lambda x: lexRank(x['source'], 10), axis=1)\n",
    "test_df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/lextR2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/lextR2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eca820",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cl_output(df):\n",
    "    return re.sub(r'\\(\\<\\b\\Sentence\\b: |(\\>,)|(>\\))|( \\<\\b\\Sentence\\b: )','' ,df)\n",
    "\n",
    "test_df['lex1'] = test_df.apply(lambda x: cl_output(str(x['lex1'])), axis=1)\n",
    "test_df['lex2'] = test_df.apply(lambda x: cl_output(str(x['lex2'])), axis=1)\n",
    "test_df['lex5'] = test_df.apply(lambda x: cl_output(str(x['lex5'])), axis=1)\n",
    "test_df['lex10'] = test_df.apply(lambda x: cl_output(str(x['lex10'])), axis=1)\n",
    "test_df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/lextR2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import sys\n",
    "\n",
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/test.txt\", \"w\")\n",
    "\n",
    "hy, rf = test_df['lex1'], test_df['summary']\n",
    "rouge= Rouge()\n",
    "scoresT1= rouge.get_scores(hy, rf, avg=True)\n",
    "print(\"LextRank1 scores: \", scoresT1, '\\n\\n')\n",
    "\n",
    "hy2 = test_df['lex2']\n",
    "scoresT2= rouge.get_scores(hy2, rf, avg=True)\n",
    "print(\"LextRank2 scores: \", scoresT2, '\\n\\n')\n",
    "\n",
    "hy5 = test_df['lex5']\n",
    "scoresT5= rouge.get_scores(hy5, rf, avg=True)\n",
    "print(\"LextRank5 scores: \", scoresT5, '\\n\\n')\n",
    "\n",
    "hy10 = test_df['lex10']\n",
    "scoresT10= rouge.get_scores(hy10, rf, avg=True)\n",
    "print(\"LextRank10 scores: \", scoresT10, '\\n\\n')\n",
    "\n",
    "sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
