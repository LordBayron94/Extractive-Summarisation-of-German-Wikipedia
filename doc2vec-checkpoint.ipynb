{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlK_BOWYRrAE",
    "outputId": "85808f08-b175-4c16-da6c-679f620e8795"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Data cleaning:\n",
    "import re\n",
    "from collections import Counter \n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "#from spellchecker import SpellChecker\n",
    "# Modeling basics:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# For D2V + RF:\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import utils\n",
    "# For Glove + LSTM:\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "co-gKT_cINsr",
    "outputId": "79a8d00a-8463-4798-aa35-7ad64c74eb14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source1</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>sim_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nach Schulabschluss studierte Universität Hul...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1978 drehte ersten Kurzfilm .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Seit 1981 Autor Story Editor tätig .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Er wurde Theaterstücken , Rundfunkhörspielen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513930</th>\n",
       "      <td>3513930</td>\n",
       "      <td>Carl Sigmans Text erschien 1960er Jahren briti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513931</th>\n",
       "      <td>3513931</td>\n",
       "      <td>Camillo Felgen alias Heinz Helmer verfasste de...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513932</th>\n",
       "      <td>3513932</td>\n",
       "      <td>Zahlreiche Interpreten sangen 1964 Lied franzo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513933</th>\n",
       "      <td>3513933</td>\n",
       "      <td>Von Letzterer stammt spanischsprachige Fassung...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513934</th>\n",
       "      <td>3513934</td>\n",
       "      <td>In 1970er Jahren nahmen mehrere prominente Sä...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3513935 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                            source1  \\\n",
       "0                 0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1                 1  Nach Schulabschluss studierte Universität Hul...   \n",
       "2                 2                      1978 drehte ersten Kurzfilm .   \n",
       "3                 3              Seit 1981 Autor Story Editor tätig .   \n",
       "4                 4  Er wurde Theaterstücken , Rundfunkhörspielen...   \n",
       "...             ...                                                ...   \n",
       "3513930     3513930  Carl Sigmans Text erschien 1960er Jahren briti...   \n",
       "3513931     3513931  Camillo Felgen alias Heinz Helmer verfasste de...   \n",
       "3513932     3513932  Zahlreiche Interpreten sangen 1964 Lied franzo...   \n",
       "3513933     3513933  Von Letzterer stammt spanischsprachige Fassung...   \n",
       "3513934     3513934  In 1970er Jahren nahmen mehrere prominente Sä...   \n",
       "\n",
       "         no_words_inSent_SS  no_words_inSent_SK  noCap_LetterWords_inSentence  \\\n",
       "0                         1                   0                             7   \n",
       "1                         0                   0                             6   \n",
       "2                         0                   0                             1   \n",
       "3                         0                   0                             4   \n",
       "4                         0                   0                             9   \n",
       "...                     ...                 ...                           ...   \n",
       "3513930                   0                   0                             7   \n",
       "3513931                   5                   1                            12   \n",
       "3513932                   1                   1                            11   \n",
       "3513933                   2                   1                             6   \n",
       "3513934                   5                   1                            13   \n",
       "\n",
       "         sim_sent  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "3513930         1  \n",
       "3513931         0  \n",
       "3513932         1  \n",
       "3513933         0  \n",
       "3513934         1  \n",
       "\n",
       "[3513935 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_class.csv', encoding='utf-8')\n",
    "df = df.drop(['Unnamed: 0.1'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sim_sent'] = np.where((df.noCap_LetterWords_inSentence>8.0) | (df.no_words_inSent_SS>=2.0), 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nbVhTpibIY1U"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_g2TG4FsR0-k",
    "outputId": "8349b77a-e1c0-46ff-e687-0ca87c2392df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['source1']), tags=[r.sim_sent]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['source1']), tags=[r.sim_sent]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_IfD82qmtwk_"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUW3g91ftx79",
    "outputId": "6df64607-4afb-4200-d4dd-84b8df8b611c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2983263.07it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SSyi3LQt_XD",
    "outputId": "2b6a3b83-4c83-406c-d5e2-97074b98da92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3040427.83it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3096327.20it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3152704.06it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3131724.54it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3090559.76it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3040517.43it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3122300.79it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3066502.41it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3084573.05it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2976352.95it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2774272.44it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2946218.52it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2970593.97it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2990214.89it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3012238.51it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2975131.59it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2926417.47it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3028638.19it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3002240.43it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2963653.80it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2984392.70it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3066440.43it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 2999046.25it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3101056.97it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3010386.58it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3191131.25it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3095893.29it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3042797.85it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3079102.88it/s]\n",
      "100%|██████████| 2459754/2459754 [00:00<00:00, 3021794.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HgdVwfPluDyk"
   },
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNLEoGnWuL8O"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "lr_classifier = LogisticRegression(n_jobs=1, C=1e5,random_state=1650)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "print(\"\\n============ Logistic regression\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw9YHovDw4Ob"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(X_train, y_train)\n",
    "y_pred = gnb_classifier.predict(X_test)\n",
    "print(\"\\n============ Naive Bayes\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDtSzelow62c"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_classifier = RandomForestClassifier()\n",
    "forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = forest_classifier.predict(X_test)\n",
    "print(\"\\n============ Random Forest\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQsmK8Nkw_Db"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_classifier = svm.LinearSVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(\"\\n============ Support Vector Machine\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "doc2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
