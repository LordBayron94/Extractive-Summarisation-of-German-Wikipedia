{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f29ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re, nltk, string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd683248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a856ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in nltk.word_tokenize(text) if word not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "df[\"source\"] = df[\"source\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1148f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85,stop_words=german_stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(df['source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f4a1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971eef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.49006738, 2.90718032, 5.39087558, ..., 6.64081768, 6.76036283,\n",
       "       6.99548257])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744bee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77d4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fbedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = df['source'][0]\n",
    "def keyw(df):\n",
    "    kljuc = []\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([df]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    for k in keywords:\n",
    "        kljuc.append(k)\n",
    "    return kljuc\n",
    "\n",
    "df['keywords'] = df['source'].apply(keyw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['source'][56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d61a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Minghella', 'Sohn', 'italienisch-schottischer', 'Eltern', ',', 'Isle', 'of', 'Wight', 'Fabrik', 'fuÌˆr', 'Eiscreme', 'betrieben', '.']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-59864c15e5f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords_inSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-59864c15e5f2>\u001b[0m in \u001b[0;36mkeywords_inSent\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"german\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def keywords_inSent(df):\n",
    "    text = []\n",
    "    sentance= sent_tokenize(df, language=\"german\")\n",
    "    for s in sentance:\n",
    "        words = word_tokenize(s, language=\"german\")\n",
    "        print(words)\n",
    "        text.append(len(set.intersection(set(words), set(df['keywords']))))\n",
    "    return text\n",
    "\n",
    "print(keywords_inSent(df['source'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b618bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = df['source'][2568]\n",
    "def get_kew(df):\n",
    "    kw=[]\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([df]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    for k in keywords:\n",
    "        kw.append(k)\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f9e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(idx):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([df['source'][idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54679c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>Anthony Minghella, CBE war ein britischer Film...</td>\n",
       "      <td>[oscar, regie, opera, bbc, film, bestes, ferns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ende 1940er Jahre wurde erste Auteur-Theorie f...</td>\n",
       "      <td>Die Auteur-Theorie ist eine Filmtheorie und di...</td>\n",
       "      <td>[theorie, film, filme, autor, regisseur, ansat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino , geboren Manhattan , Sohn Salvatore...</td>\n",
       "      <td>Alfredo James \"Al\" Pacino ist ein US-amerikani...</td>\n",
       "      <td>[theatre, al, new, the, theaterstu, richard, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Name Alkalimetalle leitet arabischen Wort ...</td>\n",
       "      <td>Als Alkalimetalle werden die chemischen Elemen...</td>\n",
       "      <td>[sungen, wasser, eigenschaften, reaktion, lo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Arbeit bereits seit Altertum Gegenstand re...</td>\n",
       "      <td>Das deutsche Arbeitsrecht ist ein Rechtsgebiet...</td>\n",
       "      <td>[geregelt, ra, gewerkschaften, bgb, betrieben,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1  Ende 1940er Jahre wurde erste Auteur-Theorie f...   \n",
       "2  Al Pacino , geboren Manhattan , Sohn Salvatore...   \n",
       "3  Der Name Alkalimetalle leitet arabischen Wort ...   \n",
       "4  Die Arbeit bereits seit Altertum Gegenstand re...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Anthony Minghella, CBE war ein britischer Film...   \n",
       "1  Die Auteur-Theorie ist eine Filmtheorie und di...   \n",
       "2  Alfredo James \"Al\" Pacino ist ein US-amerikani...   \n",
       "3  Als Alkalimetalle werden die chemischen Elemen...   \n",
       "4  Das deutsche Arbeitsrecht ist ein Rechtsgebiet...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [oscar, regie, opera, bbc, film, bestes, ferns...  \n",
       "1  [theorie, film, filme, autor, regisseur, ansat...  \n",
       "2  [theatre, al, new, the, theaterstu, richard, y...  \n",
       "3  [sungen, wasser, eigenschaften, reaktion, lo, ...  \n",
       "4  [geregelt, ra, gewerkschaften, bgb, betrieben,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords'] = df['source'].apply(get_kew)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f565068",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=120\n",
    "keywords=get_keywords(idx)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a06ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def no_keywords(df):\n",
    "    sent = sent_tokenize(df, language=\"german\")\n",
    "    for s in sent:\n",
    "        words\n",
    "\n",
    "df[['word_overlap', 'overlap_count']] = df.apply(no_keywords, axis=1, raw=True).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sent_tokenize(df['source'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e88ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "sentance= sent_tokenize(df['source'][0], language=\"german\")\n",
    "for s in sentance:\n",
    "    words = word_tokenize(s, language=\"german\")\n",
    "    print([sum(set.intersection(set(w), set(df['keywords'][0]))) for w in words])\n",
    "    text.append([sum(set.intersection(set(w), set(df['keywords'][0]))) for w in words])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "448379f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1fb0d76ac8e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"keyw_in_sent\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeywords_inSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7766\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7767\u001b[0m         )\n\u001b[1;32m-> 7768\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-1fb0d76ac8e6>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"keyw_in_sent\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeywords_inSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-1fb0d76ac8e6>\u001b[0m in \u001b[0;36mkeywords_inSent\u001b[1;34m(df, keywords)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"keyw_in_sent\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeywords_inSent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def keywords_inSent(df, keywords):\n",
    "    text=[]\n",
    "    sentance= sent_tokenize(df, language=\"german\")\n",
    "    for s in sentance:\n",
    "        s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "        text.append(list(len(set(s.split()).intersection(set(keywords)))))\n",
    "    return text\n",
    "df[\"keyw_in_sent\"] = df.apply(lambda x: keywords_inSent(x[\"source\"], x['keywords']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ffcd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_lWords(df):\n",
    "    tokens = sent_tokenize(df, language=\"german\")\n",
    "    return [sum([c.isupper() for c in a]) for a in tokens]\n",
    "\n",
    "df['noCap_LetterWords_inSentence'] = df['source'].apply(capital_lWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['keywords'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b54b495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>Anthony Minghella, CBE war ein britischer Film...</td>\n",
       "      <td>[oscar, regie, opera, bbc, film, bestes, ferns...</td>\n",
       "      <td>[7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ende 1940er Jahre wurde erste Auteur-Theorie f...</td>\n",
       "      <td>Die Auteur-Theorie ist eine Filmtheorie und di...</td>\n",
       "      <td>[theorie, film, filme, autor, regisseur, ansat...</td>\n",
       "      <td>[10, 7, 7, 6, 4, 3, 5, 5, 8, 2, 3, 8, 13, 6, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino , geboren Manhattan , Sohn Salvatore...</td>\n",
       "      <td>Alfredo James \"Al\" Pacino ist ein US-amerikani...</td>\n",
       "      <td>[theatre, al, new, the, theaterstu, richard, y...</td>\n",
       "      <td>[15, 3, 13, 8, 7, 2, 3, 17, 8, 4, 11, 3, 10, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Name Alkalimetalle leitet arabischen Wort ...</td>\n",
       "      <td>Als Alkalimetalle werden die chemischen Elemen...</td>\n",
       "      <td>[sungen, wasser, eigenschaften, reaktion, lo, ...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 4, 2, 5, 3, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Arbeit bereits seit Altertum Gegenstand re...</td>\n",
       "      <td>Das deutsche Arbeitsrecht ist ein Rechtsgebiet...</td>\n",
       "      <td>[geregelt, ra, gewerkschaften, bgb, betrieben,...</td>\n",
       "      <td>[5, 5, 5, 8, 3, 3, 6, 8, 6, 7, 8, 6, 4, 8, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1  Ende 1940er Jahre wurde erste Auteur-Theorie f...   \n",
       "2  Al Pacino , geboren Manhattan , Sohn Salvatore...   \n",
       "3  Der Name Alkalimetalle leitet arabischen Wort ...   \n",
       "4  Die Arbeit bereits seit Altertum Gegenstand re...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Anthony Minghella, CBE war ein britischer Film...   \n",
       "1  Die Auteur-Theorie ist eine Filmtheorie und di...   \n",
       "2  Alfredo James \"Al\" Pacino ist ein US-amerikani...   \n",
       "3  Als Alkalimetalle werden die chemischen Elemen...   \n",
       "4  Das deutsche Arbeitsrecht ist ein Rechtsgebiet...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [oscar, regie, opera, bbc, film, bestes, ferns...   \n",
       "1  [theorie, film, filme, autor, regisseur, ansat...   \n",
       "2  [theatre, al, new, the, theaterstu, richard, y...   \n",
       "3  [sungen, wasser, eigenschaften, reaktion, lo, ...   \n",
       "4  [geregelt, ra, gewerkschaften, bgb, betrieben,...   \n",
       "\n",
       "                        noCap_LetterWords_inSentence  \n",
       "0  [7, 6, 1, 4, 9, 9, 10, 8, 17, 37, 11, 7, 12, 7...  \n",
       "1  [10, 7, 7, 6, 4, 3, 5, 5, 8, 2, 3, 8, 13, 6, 6...  \n",
       "2  [15, 3, 13, 8, 7, 2, 3, 17, 8, 4, 11, 3, 10, 1...  \n",
       "3  [8, 7, 2, 3, 5, 2, 2, 2, 6, 3, 4, 2, 5, 3, 5, ...  \n",
       "4  [5, 5, 5, 8, 3, 3, 6, 8, 6, 7, 8, 6, 4, 8, 5, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(columns):\n",
    "    f_desc, f_def = columns[0], columns[1]\n",
    "    common = set(f_desc.split()).intersection(set(f_def.split()))\n",
    "    return len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overlap_count'] = df.apply(f, axis=1, raw=True).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a843959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "text2 = df['summary'][0]\n",
    "text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = sent_tokenize(df['source'][0], language = \"german\")\n",
    "tok2 = word_tokenize(text2, language = \"german\")\n",
    "print(len(set(word_tokenize(tokens[0], language=\"german\")).intersection(set(tok2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc95c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_sourceSummary(df, df2):\n",
    "    f = []\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len((set(s.split()).intersection(set(tok2)))))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7988d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_sourceKeywords(df, df2):\n",
    "    f = []\n",
    "    df.lower()\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len((set(s.split()).intersection(set(tok2)))))\n",
    "    return f\n",
    "\n",
    "def listToString(df):\n",
    "    string = ' '.join([str(e) for e in df])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords'] = df['keywords'].apply(listToString)\n",
    "df['no_words_inSent_SK'] = df.apply(lambda x: W_sourceKeywords(x['source'], x['keywords']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['keywords'] = df['keywords'].apply(listToString)\n",
    "print(df['no_words_inSent_SK'][0], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='keywords2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a132ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_words_inSent_SS'] = df.apply(lambda x: W_sourceSummary(x['source'], x['summary']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a29b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = sent_tokenize(df['source'][0], language=\"german\")\n",
    "print(tok2, \"\\n\", \"\\n\", tok1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
