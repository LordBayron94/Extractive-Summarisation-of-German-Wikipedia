{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "7b895315-a7ef-46c4-8801-1232a42e261e",
    "_uuid": "c3aa0876-8705-4ab3-95a8-b9601e032626",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:02.170444Z",
     "iopub.status.busy": "2021-08-19T22:59:02.170087Z",
     "iopub.status.idle": "2021-08-19T22:59:19.152160Z",
     "shell.execute_reply": "2021-08-19T22:59:19.150885Z",
     "shell.execute_reply.started": "2021-08-19T22:59:02.170401Z"
    },
    "id": "yCDy0hNIqwjv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a6bf28ff-92f0-401f-ae54-e63a467d50c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (0.23.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 810 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.6.3)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\n",
      "pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.2 which is incompatible.\n",
      "hypertools 0.7.0 requires scikit-learn!=0.22,<0.24,>=0.19.1, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
      "Successfully installed scikit-learn-0.24.2\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os, networkx as nx, collections, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from HanTa import HanoverTagger as ht\n",
    "import nltk, string\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics, neighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "nltk.download('wordnet')\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "nltk.download('wordnet') # first-time use only\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "!pip install -U scikit-learn\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "df3618da-88b6-4bb8-951a-8170f60a3706",
    "_uuid": "3eeca152-64d2-42df-a77e-c4185668b0b8",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:19.154691Z",
     "iopub.status.busy": "2021-08-19T22:59:19.154234Z",
     "iopub.status.idle": "2021-08-19T22:59:34.216276Z",
     "shell.execute_reply": "2021-08-19T22:59:34.215288Z",
     "shell.execute_reply.started": "2021-08-19T22:59:19.154642Z"
    },
    "id": "StIw5DdwrHy_",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "34279dab-5a00-407a-87d9-35f542ffe8d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source1</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>noCap_LetterWords_inSentence</th>\n",
       "      <th>sim_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nach Schulabschluss studierte Universität Hul...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1978 drehte ersten Kurzfilm .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Seit 1981 Autor Story Editor tätig .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Er wurde Theaterstücken , Rundfunkhörspielen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513930</th>\n",
       "      <td>3513930</td>\n",
       "      <td>Carl Sigmans Text erschien 1960er Jahren briti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513931</th>\n",
       "      <td>3513931</td>\n",
       "      <td>Camillo Felgen alias Heinz Helmer verfasste de...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513932</th>\n",
       "      <td>3513932</td>\n",
       "      <td>Zahlreiche Interpreten sangen 1964 Lied franzo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513933</th>\n",
       "      <td>3513933</td>\n",
       "      <td>Von Letzterer stammt spanischsprachige Fassung...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513934</th>\n",
       "      <td>3513934</td>\n",
       "      <td>In 1970er Jahren nahmen mehrere prominente Sä...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3513935 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                            source1  \\\n",
       "0                 0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1                 1  Nach Schulabschluss studierte Universität Hul...   \n",
       "2                 2                      1978 drehte ersten Kurzfilm .   \n",
       "3                 3              Seit 1981 Autor Story Editor tätig .   \n",
       "4                 4  Er wurde Theaterstücken , Rundfunkhörspielen...   \n",
       "...             ...                                                ...   \n",
       "3513930     3513930  Carl Sigmans Text erschien 1960er Jahren briti...   \n",
       "3513931     3513931  Camillo Felgen alias Heinz Helmer verfasste de...   \n",
       "3513932     3513932  Zahlreiche Interpreten sangen 1964 Lied franzo...   \n",
       "3513933     3513933  Von Letzterer stammt spanischsprachige Fassung...   \n",
       "3513934     3513934  In 1970er Jahren nahmen mehrere prominente Sä...   \n",
       "\n",
       "         no_words_inSent_SS  no_words_inSent_SK  noCap_LetterWords_inSentence  \\\n",
       "0                         1                   0                             7   \n",
       "1                         0                   0                             6   \n",
       "2                         0                   0                             1   \n",
       "3                         0                   0                             4   \n",
       "4                         0                   0                             9   \n",
       "...                     ...                 ...                           ...   \n",
       "3513930                   0                   0                             7   \n",
       "3513931                   5                   1                            12   \n",
       "3513932                   1                   1                            11   \n",
       "3513933                   2                   1                             6   \n",
       "3513934                   5                   1                            13   \n",
       "\n",
       "         sim_sent  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "3513930         1  \n",
       "3513931         0  \n",
       "3513932         1  \n",
       "3513933         0  \n",
       "3513934         1  \n",
       "\n",
       "[3513935 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/swisstext-german/data_class.csv', encoding='utf-8')\n",
    "df = df.drop(['Unnamed: 0.1'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "de97547d-e931-45af-9c17-f012cde096aa",
    "_uuid": "2c983918-b7a5-4b4c-a7ad-1ef4927e8aac",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:34.218492Z",
     "iopub.status.busy": "2021-08-19T22:59:34.218139Z",
     "iopub.status.idle": "2021-08-19T22:59:34.738950Z",
     "shell.execute_reply": "2021-08-19T22:59:34.737793Z",
     "shell.execute_reply.started": "2021-08-19T22:59:34.218459Z"
    },
    "id": "Y-BUqkxnrI4A",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['sim_sent'] = np.where((df.noCap_LetterWords_inSentence>8.0) | (df.no_words_inSent_SS>=2.0), 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a70222e4-8fba-48d4-9bfb-a27c1f919e5e",
    "_uuid": "2ce04a7c-a05d-4d96-8e81-943807020023",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:34.741085Z",
     "iopub.status.busy": "2021-08-19T22:59:34.740789Z",
     "iopub.status.idle": "2021-08-19T22:59:35.512323Z",
     "shell.execute_reply": "2021-08-19T22:59:35.511244Z",
     "shell.execute_reply.started": "2021-08-19T22:59:34.741046Z"
    },
    "id": "63mK1NWJ4S7x",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values('Unnamed: 0', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "07094885-c909-4ca8-a403-bfe8e5533f35",
    "_uuid": "2b76ffee-a752-4ce9-83dd-8c1726dbd710",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:35.514268Z",
     "iopub.status.busy": "2021-08-19T22:59:35.513847Z",
     "iopub.status.idle": "2021-08-19T22:59:35.683229Z",
     "shell.execute_reply": "2021-08-19T22:59:35.682177Z",
     "shell.execute_reply.started": "2021-08-19T22:59:35.514224Z"
    },
    "id": "aWfk7WCXrM3y",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2=df[0:int(len(df)/2)]\n",
    "msk = np.random.rand(len(df2)) < 0.7\n",
    "train_df = df2[msk]\n",
    "test_df = df2.iloc[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5525694c-6aa2-4cee-b81e-5d447bfe1cd8",
    "_uuid": "d715651f-17e5-41e5-afb2-3e13c8a1e09e",
    "id": "SEMOgwQ-5vAm",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "37da4bad-5e33-45a4-ccdf-217e1da21466"
   },
   "outputs": [],
   "source": [
    "n_pos_train = sum(train_df['sim_sent'] == 'Yes')\n",
    "print('Training set contains {:.2%} positive reviews'.format(n_pos_train/len(train_df)))\n",
    "n_pos_test = sum(test_df['sim_sent'] == 'Yes')\n",
    "print('Test set contains {:.2%} positive reviews'.format(n_pos_test/len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "14e18c0c-6265-47df-a3ec-07dee6e1400f",
    "_uuid": "0423678a-d3fd-4363-b367-0f90aa8b75e5",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:35.684897Z",
     "iopub.status.busy": "2021-08-19T22:59:35.684613Z",
     "iopub.status.idle": "2021-08-19T22:59:35.694678Z",
     "shell.execute_reply": "2021-08-19T22:59:35.693632Z",
     "shell.execute_reply.started": "2021-08-19T22:59:35.684873Z"
    },
    "id": "VSbT09Ui6ehS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    sentence_text = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "    \n",
    "    # convert to lower case and split at whitespace\n",
    "    words = sentence_text.lower().split()\n",
    "    \n",
    "    # remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopword_list)\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "4efeae0b-cd94-4ca3-8382-7db2095d140d",
    "_uuid": "c8ab1a4f-8c26-4773-ad58-7d324bbc38cb",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:35.696377Z",
     "iopub.status.busy": "2021-08-19T22:59:35.696002Z",
     "iopub.status.idle": "2021-08-19T22:59:35.703039Z",
     "shell.execute_reply": "2021-08-19T22:59:35.701868Z",
     "shell.execute_reply.started": "2021-08-19T22:59:35.696350Z"
    },
    "id": "ETEDZy7F7FML",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def text_to_sentences(sentence, remove_stopwords=False):\n",
    "    # use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = sent_tokenize(sentence, language = \"german\")\n",
    "\n",
    "    # each sentence is furthermore split into words\n",
    "    sentences = []    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(sentence_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "93ca62ea-3106-455a-9c4c-6d904a6d7957",
    "_uuid": "bb775d7b-4f78-4b22-b85f-aebb0d97e409",
    "execution": {
     "iopub.execute_input": "2021-08-19T22:59:35.705720Z",
     "iopub.status.busy": "2021-08-19T22:59:35.705371Z",
     "iopub.status.idle": "2021-08-19T23:01:01.578246Z",
     "shell.execute_reply": "2021-08-19T23:01:01.577475Z",
     "shell.execute_reply.started": "2021-08-19T22:59:35.705688Z"
    },
    "id": "-wL23Utm7vp-",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_sentences = []  # Initialize an empty list of sentences\n",
    "for sent in train_df['source1']:\n",
    "    train_sentences += text_to_sentences(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "335a5665-65c8-418b-8e45-e99967551866",
    "_uuid": "c635092b-d93b-4759-a97d-2298ed1ceeb1",
    "id": "VhchQaL5-QeX",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "67d67b4a-1ed0-4458-9b9a-ec861ab86072",
    "_uuid": "556827a8-ffe7-451c-a3f0-725b94106740",
    "execution": {
     "iopub.execute_input": "2021-08-19T23:01:01.580015Z",
     "iopub.status.busy": "2021-08-19T23:01:01.579610Z",
     "iopub.status.idle": "2021-08-19T23:03:54.572696Z",
     "shell.execute_reply": "2021-08-19T23:03:54.571571Z",
     "shell.execute_reply.started": "2021-08-19T23:01:01.579986Z"
    },
    "id": "3lO0AKSD-FDl",
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "model_name = 'train_model'\n",
    "# Set values for various word2vec parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 3       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "if not os.path.exists(model_name): \n",
    "    # Initialize and train the model (this will take some time)\n",
    "    model = word2vec.Word2Vec(train_sentences, workers=num_workers, \\\n",
    "                vector_size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "1589b6fe-3f5c-4ad2-93e7-18b63e4d63bc",
    "_uuid": "1f9e85a7-7533-4219-b260-7750b904598c",
    "execution": {
     "iopub.execute_input": "2021-08-19T23:03:54.574578Z",
     "iopub.status.busy": "2021-08-19T23:03:54.574278Z",
     "iopub.status.idle": "2021-08-19T23:03:54.587769Z",
     "shell.execute_reply": "2021-08-19T23:03:54.586553Z",
     "shell.execute_reply.started": "2021-08-19T23:03:54.574549Z"
    },
    "id": "ULGX1Rdz_z6c",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all reviews\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "        #print(review)\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "96267fa8-6eae-4ce1-b71a-e5c3808a0aa2",
    "_uuid": "edd2e385-d428-4f9a-9f9f-7fb5270277d1",
    "execution": {
     "iopub.execute_input": "2021-08-19T23:03:54.589861Z",
     "iopub.status.busy": "2021-08-19T23:03:54.589358Z",
     "iopub.status.idle": "2021-08-20T00:00:29.487841Z",
     "shell.execute_reply": "2021-08-20T00:00:29.486529Z",
     "shell.execute_reply.started": "2021-08-19T23:03:54.589824Z"
    },
    "id": "RMnnkTFiAPS6",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "79fc05f9-17ec-47a5-f603-24919004c810"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clean_train_sent = []\n",
    "for sent in train_df['source1']:\n",
    "    clean_train_sent.append(sentence_to_wordlist(sent, remove_stopwords=True))\n",
    "trainDataVecs = get_avg_feature_vecs(clean_train_sent, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "9ecae24c-ade2-424d-8b86-5050777aa216",
    "_uuid": "651424da-fdc4-46ad-80e7-ac7176d33126",
    "execution": {
     "iopub.execute_input": "2021-08-20T00:00:29.489849Z",
     "iopub.status.busy": "2021-08-20T00:00:29.489398Z",
     "iopub.status.idle": "2021-08-20T00:00:52.873441Z",
     "shell.execute_reply": "2021-08-20T00:00:52.872187Z",
     "shell.execute_reply.started": "2021-08-20T00:00:29.489807Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 38622 instances from train set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(trainDataVecs))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from train set.'.format(len(nan_indices)))\n",
    "    trainDataVecs = np.delete(trainDataVecs, nan_indices, axis=0)\n",
    "    train_df.drop(train_df.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert trainDataVecs.shape[0] == len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "29785627-4f22-4bb9-b3e2-2e677f411e23",
    "_uuid": "4c6efeac-1cfa-4b3a-9338-93b6b77ec101",
    "execution": {
     "iopub.execute_input": "2021-08-20T00:00:52.875683Z",
     "iopub.status.busy": "2021-08-20T00:00:52.875197Z",
     "iopub.status.idle": "2021-08-20T00:25:02.029272Z",
     "shell.execute_reply": "2021-08-20T00:25:02.028306Z",
     "shell.execute_reply.started": "2021-08-20T00:00:52.875638Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test_df['source1']:\n",
    "    clean_test_reviews.append(sentence_to_wordlist(review, remove_stopwords=True))\n",
    "testDataVecs = get_avg_feature_vecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "43e0325c-00a6-4a7f-bf01-ef84dc33032f",
    "_uuid": "cc5729b0-0686-4a23-8d32-52f7d6eb82b4",
    "execution": {
     "iopub.execute_input": "2021-08-20T00:25:02.030832Z",
     "iopub.status.busy": "2021-08-20T00:25:02.030529Z",
     "iopub.status.idle": "2021-08-20T00:25:11.954161Z",
     "shell.execute_reply": "2021-08-20T00:25:11.953178Z",
     "shell.execute_reply.started": "2021-08-20T00:25:02.030801Z"
    },
    "id": "Awchbi4UQQqx",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 16509 instances from test set.\n"
     ]
    }
   ],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(testDataVecs))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    testDataVecs = np.delete(testDataVecs, nan_indices, axis=0)\n",
    "    test_df.drop(test_df.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert testDataVecs.shape[0] == len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "8e0ed1ac-6937-4fa9-b1d3-e952be58128a",
    "_uuid": "b6228b89-66b3-4efe-a896-c9d0e4ae8e83",
    "execution": {
     "iopub.execute_input": "2021-08-20T00:25:11.956478Z",
     "iopub.status.busy": "2021-08-20T00:25:11.956153Z",
     "iopub.status.idle": "2021-08-20T00:25:12.318342Z",
     "shell.execute_reply": "2021-08-20T00:25:12.316767Z",
     "shell.execute_reply.started": "2021-08-20T00:25:11.956444Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b4ec31de-d11e-498b-8523-c2c2ffb2108d",
    "_uuid": "e5c0b9dc-5ed7-404d-99b8-6457996c112a",
    "execution": {
     "iopub.execute_input": "2021-08-20T00:25:12.320171Z",
     "iopub.status.busy": "2021-08-20T00:25:12.319881Z",
     "iopub.status.idle": "2021-08-20T00:26:21.157829Z",
     "shell.execute_reply": "2021-08-20T00:26:21.156523Z",
     "shell.execute_reply.started": "2021-08-20T00:25:12.320131Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Naive Bayes\n",
      "---- confusion_matrix:\n",
      "[[150643 148288]\n",
      " [ 44267 167317]]\n",
      "---- classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.77      0.50      0.61    298931\n",
      "         Yes       0.53      0.79      0.63    211584\n",
      "\n",
      "    accuracy                           0.62    510515\n",
      "   macro avg       0.65      0.65      0.62    510515\n",
      "weighted avg       0.67      0.62      0.62    510515\n",
      "\n",
      "---- accuracy_score:\n",
      "0.6228220522413641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(trainDataVecs, train_df['sim_sent'])\n",
    "y_pred = gnb_classifier.predict(testDataVecs)\n",
    "print(\"\\n============ Naive Bayes\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(test_df['sim_sent'],y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(test_df['sim_sent'],y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(test_df['sim_sent'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d470ed1f-1078-4260-ad16-5cab61c7c148",
    "_uuid": "90e8b281-47e5-4035-95c2-d3c8a780a034",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(random_state=1650)\n",
    "lr_classifier.fit(trainDataVecs, train_df['sim_sent'])\n",
    "y_pred = lr_classifier.predict(testDataVecs)\n",
    "print(\"\\n============ Logistic regression\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(test_df['sim_sent'],y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(test_df['sim_sent'],y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(test_df['sim_sent'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "dea85362-84c7-44d1-8fb5-8ebc4f9fa6f0",
    "_uuid": "2e6e5f6b-538d-4f0b-89d0-67e32ce6b30a",
    "execution": {
     "iopub.execute_input": "2021-08-20T00:26:21.159846Z",
     "iopub.status.busy": "2021-08-20T00:26:21.159410Z",
     "iopub.status.idle": "2021-08-20T00:29:37.593214Z",
     "shell.execute_reply": "2021-08-20T00:29:37.592266Z",
     "shell.execute_reply.started": "2021-08-20T00:26:21.159793Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Support Vector Machine\n",
      "---- confusion_matrix:\n",
      "[[241351  57580]\n",
      " [123988  87596]]\n",
      "---- classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.66      0.81      0.73    298931\n",
      "         Yes       0.60      0.41      0.49    211584\n",
      "\n",
      "    accuracy                           0.64    510515\n",
      "   macro avg       0.63      0.61      0.61    510515\n",
      "weighted avg       0.64      0.64      0.63    510515\n",
      "\n",
      "---- accuracy_score:\n",
      "0.6443434570972449\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_classifier = svm.LinearSVC()\n",
    "svm_classifier.fit(trainDataVecs, train_df['sim_sent'])\n",
    "\n",
    "y_pred = svm_classifier.predict(testDataVecs)\n",
    "print(\"\\n============ Support Vector Machine\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(test_df['sim_sent'],y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(test_df['sim_sent'],y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(test_df['sim_sent'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-20T12:51:29.042820Z",
     "iopub.status.busy": "2021-08-20T12:51:29.042403Z",
     "iopub.status.idle": "2021-08-20T12:51:30.207429Z",
     "shell.execute_reply": "2021-08-20T12:51:30.205231Z",
     "shell.execute_reply.started": "2021-08-20T12:51:29.042736Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41c809b9a2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting a random forest to labeled training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataVecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sim_sent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestDataVecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "forest = RandomForestClassifier(n_estimators = 200, random_state=450)\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train_df['sim_sent'])\n",
    "result = forest.predict(testDataVecs)\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(test_df['sim_sent'],result))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(test_df['sim_sent'], result))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(test_df['sim_sent'], result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
