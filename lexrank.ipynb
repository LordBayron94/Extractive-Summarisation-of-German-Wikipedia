{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6b4bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: treetaggerwrapper in h:\\anaconda\\lib\\site-packages (2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "H:\\anaconda\\lib\\site-packages\\treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HanTa in h:\\anaconda\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: lexrank in h:\\anaconda\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in h:\\anaconda\\lib\\site-packages (from lexrank) (1.6.2)\n",
      "Requirement already satisfied: path.py>=10.5 in h:\\anaconda\\lib\\site-packages (from lexrank) (12.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in h:\\anaconda\\lib\\site-packages (from lexrank) (0.17.3)\n",
      "Requirement already satisfied: regex>=2017.11.9 in h:\\anaconda\\lib\\site-packages (from lexrank) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in h:\\anaconda\\lib\\site-packages (from lexrank) (1.20.1)\n",
      "Requirement already satisfied: urlextract>=0.7 in h:\\anaconda\\lib\\site-packages (from lexrank) (1.3.0)\n",
      "Requirement already satisfied: path in h:\\anaconda\\lib\\site-packages (from path.py>=10.5->lexrank) (15.1.2)\n",
      "Requirement already satisfied: idna in h:\\anaconda\\lib\\site-packages (from urlextract>=0.7->lexrank) (2.10)\n",
      "Requirement already satisfied: appdirs in h:\\anaconda\\lib\\site-packages (from urlextract>=0.7->lexrank) (1.4.4)\n",
      "Requirement already satisfied: filelock in h:\\anaconda\\lib\\site-packages (from urlextract>=0.7->lexrank) (3.0.12)\n",
      "Requirement already satisfied: uritools in h:\\anaconda\\lib\\site-packages (from urlextract>=0.7->lexrank) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install treetaggerwrapper\n",
    "import os, treetaggerwrapper, networkx as nx, seaborn as sns\n",
    "!pip install HanTa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from HanTa import HanoverTagger as ht\n",
    "import nltk\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "!pip install lexrank\n",
    "from lexrank import STOPWORDS, LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8310902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexRank(documents, user_threshold, Usummary_size, fastPowerMethod):\n",
    "    lxr = LexRank(documents, stopwords=STOPWORDS['de'])\n",
    "    summary = lxr.get_summary(df, summary_size=Usummary_size,threshold = user_threshold, fast_power_method = fastPowerMethod)\n",
    "    return summary\n",
    "\n",
    "def plotLexRank(documents, user_threshold, Usummary_size, fastPowerMethod):\n",
    "    # -- ploting -- #\n",
    "    lxr = LexRank(documents, stopwords=STOPWORDS['de'])\n",
    "    scores = lxr.rank_sentences(df, summary_size=Usummary_size,threshold = user_threshold, fast_power_method = fastPowerMethod)\n",
    "    sns.set(color_codes=True)\n",
    "    x = sent_scr\n",
    "    sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c62214",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents = df['source']\n",
    "lxr = LexRank(documents, stopwords=STOPWORDS['de'])\n",
    "for doc in documents:\n",
    "    sentances = df['source'][doc]\n",
    "    tokens = sent_tokenize(sentances, language='german')\n",
    "    #print(tokens)\n",
    "    df['lexrank'][doc] = lxr.get_summary(tokens, summary_size=2, threshold=.1, fast_power_method = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d74fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents = df['source']\n",
    "lxr = LexRank(documents, stopwords=STOPWORDS['de'])\n",
    "sentances = df['source'][0]\n",
    "tokens = sent_tokenize(sentances, language='german')\n",
    "#print(tokens)\n",
    "lexrank = lxr.get_summary(tokens, summary_size=5, threshold=.1, fast_power_method = True)\n",
    "print(lexrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank = lxr.get_summary(tokens, summary_size=10, threshold=.1, fast_power_method = True)\n",
    "print(lexrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank = lxr.get_summary(tokens, summary_size=2, threshold=.1, fast_power_method = True)\n",
    "print(lexrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34133d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexrank = lxr.get_summary(tokens, summary_size=1, threshold=.5, fast_power_method = True)\n",
    "print(lexrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830158f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scr = lxr.rank_sentences(tokens, threshold=.2, fast_power_method = True)\n",
    "sns.set(color_codes=True)\n",
    "x = sent_scr\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f267ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return len(tokens)\n",
    "\n",
    "df['no_sent_source'] = df['source'].apply(identify_tokens)\n",
    "df['no_sent_summary'] = df['summary'].apply(identify_tokens)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = word_tokenize(row, language='german')\n",
    "    return len(tokens)\n",
    "\n",
    "df['no_word_source'] = df['source'].apply(identify_tokens)\n",
    "df['no_word_summary'] = df['summary'].apply(identify_tokens)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fcfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w(row):\n",
    "    tokens = word_tokenize(row, language='german')\n",
    "    return float(sum(len(w) for w in tokens)/len(tokens))\n",
    "\n",
    "df['avg_source_w_len'] = df['source'].apply(avg_w)\n",
    "df['avg_summary_w_len'] = df['summary'].apply(avg_w)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/processed.csv')\n",
    "\n",
    "avg_w_len_source = float(sum(idx for idx in df['avg_word_len'])/len(df))\n",
    "avg_w_len_summary = float(sum(idx for idx in df['avg_summary_w_len'])/len(df))\n",
    "\n",
    "print(avg_w_len_source, avg_w_len_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"avg_source_w_len\": \"avg_word_len\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_s(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return float(sum(len(s) for s in tokens)/len(tokens))\n",
    "\n",
    "df['avg_sent_len_source'] = df['source'].apply(avg_s)\n",
    "df['avg_sent_len_summary'] = df['summary'].apply(avg_s)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaffee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8511b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_s_len_source = float(sum(idx for idx in df['avg_sent_len_source'])/len(df))\n",
    "avg_s_len_summary = float(sum(idx for idx in df['avg_sent_len_summary'])/len(df))\n",
    "\n",
    "print(avg_s_len_source, avg_s_len_summary)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8edd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_s_no_source = float(sum(df['no_sent_source'])/len(df['no_sent_source']))\n",
    "avg_s_no_summary = float(sum(df['no_sent_summary'])/len(df['no_sent_summary']))\n",
    "print(avg_s_no_source, avg_s_no_summary)\n",
    "\n",
    "df[df['no_sent_source'] == df['no_sent_source'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c00a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w_no_source = float(sum(df['no_word_source'])/len(df['no_word_source']))\n",
    "avg_w_no_summary = float(sum(df['no_word_summary'])/len(df['no_word_summary']))\n",
    "print(avg_w_no_source, avg_w_no_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w_len_source = float(sum(df['no_word_source'])/len(df['no_word_source']))\n",
    "avg_w_len_summary = float(sum(df['no_word_summary'])/len(df['no_word_summary']))\n",
    "print(avg_w_no_source, avg_w_no_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['no_word_summary'] == df['no_word_summary'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['no_sent_summary'] == df['no_sent_summary'].min()] & df[df['avg_summary_w_len']==df['avg_summary_w_len'].min()] & df[df['no_word_summary']==df['no_word_summary'].min()]& df[df['avg_sent_len_summary']==df['avg_sent_len_summary'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5019a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['no_sent_source'] == df['no_sent_source'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a8cde7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7f5bfc4d8251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_word_summary'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_word_summary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[df['no_word_summary'] == df['no_word_summary'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45054656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = []\n",
    "doc.extend(df['source'].tolist())\n",
    "print(doc)\n",
    "def Lr(df, size): \n",
    "    lxr = LexRank(doc, stopwords=STOPWORDS['de'])\n",
    "    tokens = sent_tokenize(df, language='german')\n",
    "    #print(tokens)\n",
    "    return lxr.get_summary(tokens, summary_size=size, threshold=.1, fast_power_method = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca8fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b231ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LexRank1'] = df.apply(lambda x: Lr(x['source'], 1), axis=1)\n",
    "df['LexRank2'] = df.apply(lambda x: Lr(x['source'], 2), axis=1)\n",
    "df['LexRank5'] = df.apply(lambda x: Lr(x['source'], 5), axis=1)\n",
    "df['LexRank10'] = df.apply(lambda x: Lr(x['source'], 10), axis=1)\n",
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/lextrank.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caddb20",
   "metadata": {},
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lexrank1']=df.apply(lambda x: Lr(x['source'], 1), axis=1)\n",
    "df['lexrank2']=df.apply(lambda x: Lr(x['source'], 2), axis=1)\n",
    "df['lexrank5']=df.apply(lambda x: Lr(x['source'], 5), axis=1)\n",
    "df['lexrank10']=df.apply(lambda x: Lr(x['source'], 10), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
