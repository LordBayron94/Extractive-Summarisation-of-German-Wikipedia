{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Desktop/GermCor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7c5a412980b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmovie_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Desktop/GermCor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovie_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovie_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[0;32m    166\u001b[0m                if isdir(join(container_path, f))]\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Desktop/GermCor'"
     ]
    }
   ],
   "source": [
    "### IMPORTING DATASET\n",
    "\n",
    "# We will use the load_files function from the sklearn_datasets library to import the dataset into our application. The load_files function automatically divides the dataset into data and target sets. The load_files will treat each folder inside the \"txt_sentoken\" folder as one category.\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "movie_data = load_files(\"../Desktop/GermCor\")\n",
    "x, y = movie_data.data, movie_data.target\n",
    "\n",
    "# In the code above, the load_files function loads the data from both \"neg\" and \"pos\" folders into the x variable, while the target categories are stored in y. Here x is a list of 2000 string type elements where each element corresponds to single user review. Similarly, y is a numpy array of size 2000. If you print y on the screen, you will see an array of 1s and 0s. This is because, for each category, the load_files function adds a number to the target numpy array. We have two categories: \"neg\" and \"pos\", therefore 1s and 0s have been added to the target array.\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### TEXT PREPROCESSING\n",
    "\n",
    "# Once the dataset has been imported, the next step is to preprocess the text. Text may contain numbers, special characters, and unwanted spaces. Depending upon the problem we face, we may or may not need to remove these special characters and numbers from text. However, for the sake of explanation, we will remove all the special characters, numbers, and unwanted spaces from our text.\n",
    "\n",
    "# First download wordnet:\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(x)):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(x[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    document = document.split()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)\n",
    "\n",
    "# In the code above we use Regex Expressions from Python re library to perform different preprocessing tasks. We start by removing all non-word characters such as special characters, numbers, etc.\n",
    "# Next, we remove all the single characters. For instance, when we remove the punctuation mark from \"David's\" and replace it with a space, we get \"David\" and a single character \"s\", which has no meaning. To remove such single characters we use \\s+[a-zA-Z]\\s+ regular expression which substitutes all the single characters having spaces on either side, with a single space.\n",
    "# Next, we use the \\^[a-zA-Z]\\s+ regular expression to replace a single character from the beginning of the document, with a single space. Replacing single characters with a single space may result in multiple spaces, which is not ideal.\n",
    "# We again use the regular expression \\s+ to replace one or more spaces with a single space. When you have a dataset in bytes format, the alphabet letter \"b\" is appended before every string. The regex ^b\\s+ removes \"b\" from the start of a string. The next step is to convert the data to lower case so that the words that are actually the same but have different cases can be treated equally.\n",
    "# The final preprocessing step is the lemmatization. In lemmatization, we reduce the word into dictionary root form. For instance \"cats\" is converted into \"cat\". Lemmatization is done in order to avoid creating features that are semantically similar but syntactically different. For instance, we don't want two different features named \"cats\" and \"cat\", which are semantically similar, therefore we perform lemmatization.\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### TEXT TO NUMBERS\n",
    "\n",
    "# Machines, unlike humans, cannot understand the raw text. Machines can only see numbers. Particularly, statistical techniques such as machine learning can only deal with numbers. Therefore, we need to convert our text into numbers.\n",
    "\n",
    "# The following code uses the bag of words model to convert text documents into corresponding numerical features:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Download stopwords corpus:\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "x = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# The code above uses CountVectorizer class from the sklearn.feature_extraction.text library.\n",
    "# The first parameter is the max_features parameter, which is set to 1500. This is because when you convert words to numbers using the bag of words approach, all the unique words in all the documents are converted into features. All the documents can contain tens of thousands of unique words. But the words that have a very low frequency of occurrence are unusually not a good parameter for classifying documents. Therefore we set the max_features parameter to 1500, which means that we want to use 1500 most occurring words as features for training our classifier.\n",
    "# The next parameter is min_df and it has been set to 5. This corresponds to the minimum number of documents that should contain this feature. So we only include those words that occur in at least 5 documents. Similarly, for the max_df, feature the value is set to 0.7; in which the fraction corresponds to a percentage. Here 0.7 means that we should include only those words that occur in a maximum of 70% of all the documents. Words that occur in almost every document are usually not suitable for classification because they do not provide any unique information about the document.\n",
    "# Finally, we remove the stop words from our text since, in the case of sentiment analysis, stop words may not contain any useful information. To remove the stop words we pass the stopwords object from the nltk.corpus library to the stop_wordsparameter.\n",
    "# The fit_transform function of the CountVectorizer class converts text documents into corresponding numeric features.\n",
    "\n",
    "# The bag of words approach works fine for converting text to numbers. However, it has one drawback. It assigns a score to a word based on its occurrence in a particular document. It doesn't take into account the fact that the word might also be having a high frequency of occurrence in other documents as well. TFIDF resolves this issue by multiplying the term frequency of a word by the inverse document frequency. The TF stands for \"Term Frequency\" while IDF stands for \"Inverse Document Frequency\".\n",
    "# The term frequency is calculated as: (Number of Occurrences of a word)/(Total words in the document)  \n",
    "# And the Inverse Document Frequency is calculated as: Log( (Total number of documents)/(Number of documents containing the word) )\n",
    "# The TFIDF value for a word in a particular document is higher if the frequency of occurrence of that word is higher in that specific document but lower in all the other documents.\n",
    "\n",
    "# The following code uses TFIDF to convert text documents into corresponding numerical features:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "x = tfidfconverter.fit_transform(documents).toarray()\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### SPLITING DATA INTO TRAINING AND TESTING SETS\n",
    "\n",
    "# Like any other supervised machine learning problem, we need to divide our data into training and testing sets:\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "perc = 0.8\n",
    "limit = math.floor(len(x)*perc)\n",
    "subset_indexes = random.sample(range( 0, len(x) ), limit)\n",
    "train_mask = np.zeros(len(x), np.bool)\n",
    "train_mask[subset_indexes] = 1\n",
    "test_mask = np.ones(len(x), np.bool)\n",
    "test_mask[subset_indexes] = 0\n",
    "\n",
    "x_train = x[train_mask]\n",
    "x_test  = x[test_mask]\n",
    "y_train = y[train_mask]\n",
    "y_test  = y[test_mask]\n",
    "\n",
    "# The above code divides data into 20% test set and 80% training set.\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### TRAINING NAIVE BAYES\n",
    "\n",
    "# To train our machine learning model using the Naive Bayes algorithm we will use GaussianNB class from the sklearn.naive_bayes library. The fit method of this class is used to train the algorithm. We need to pass the training data and training target sets to this method.\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Finally, to predict the sentiment for the documents in our test set we can use the predict method of the GaussianNB class as shown below:\n",
    "\n",
    "y_pred = gnb_classifier.predict(x_test)\n",
    "\n",
    "# print(y_pred)  # predicted sentiments\n",
    "# print(y_test)  # real sentiments\n",
    "# print(y_pred-y_test)  # zeros are correctly classified instances\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### EVALUATION\n",
    "\n",
    "# We can use confusion_matrix, classification_report, and accuracy_score utilities from the sklearn.metrics library:\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"\\n============ Naive Bayes\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### SAVING AND LOADING THE MODEL\n",
    "\n",
    "# In the script above, our machine learning model did not take much time to execute. One of the reasons for the quick training time is the fact that we had a relatively smaller training set. We had 2000 documents, of which we used 80% (1600) for training. However, in real-world scenarios, there can be millions of documents. In such cases, it can take hours or even days (if you have slower machines) to train the algorithms. Therefore, it is recommended to save the model once it is trained.\n",
    "\n",
    "# We can save our model as a pickle object in Python:\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(gnb_classifier, picklefile)\n",
    "\t\n",
    "# Once you execute the above code, you can see the text_classifier file in your working directory. We have saved our trained model and we can use it later for directly making predictions, without training.\n",
    "\n",
    "# To load the model, we can use the following code:\n",
    "\n",
    "with open('text_classifier', 'rb') as training_model:\n",
    "    gnb_classifier_loaded = pickle.load(training_model)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "### OTHER CLASSIFICATION ALGORITHMS\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(random_state=0)\n",
    "tree_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = tree_classifier.predict(x_test)\n",
    "print(\"\\n============ Decision Tree\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_classifier = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "forest_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = forest_classifier.predict(x_test)\n",
    "print(\"\\n============ Random Forest\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(gamma='scale')\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = svm_classifier.predict(x_test)\n",
    "print(\"\\n============ Support Vector Machine\")\n",
    "print(\"---- confusion_matrix:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"---- classification_report:\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"---- accuracy_score:\")\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "### ZADATAK\n",
    "\n",
    "# Kopirajte nekoliko movie review-ova sa imdb-a, te prediktajte sa nekim od algoritama dali su ti review-ovi pozitivni ili negativni.\n",
    "\n",
    "r = \"\"\"This is probably the highest 8/10 I could give for an 8/10 movie.\n",
    "\n",
    "There are some very good moments in Endgame and that's coming from a long standing MCU fan growing up with these movies. In creating an extensive and emotional journey for a lot of the MCU characters, it absolutely succeeds.\n",
    "\n",
    "Endgame ultimately is an excellent conclusion to over 10 years of film but I feel the plot's pacing and direction is really lacking to Infinity War, which 'trimmed the fat' and was much more focused and energetic. Granted, Endgame takes risks but this might be a disappointment to some fans with its slower pace that focuses on character development over action and intense sequences. The run time probably should be shorter or streamlined. Less cuts and motion blur during fight scenes would've made it easier to track fights as well.\n",
    "\n",
    "Respect for the Russo Brothers for taking the direction they did with Endgame and it really does satisfy but Infinity War is overall a more entertaining movie. It's not the best MCU movie but I can't dispute its great conclusion.\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
