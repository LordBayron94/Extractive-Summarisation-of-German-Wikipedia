{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re, nltk, numpy as np, string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42233e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>no_words_inSent_SS</th>\n",
       "      <th>no_words_inSent_SK</th>\n",
       "      <th>sim_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Minghella Sohn italienisch-schottischer Eltern...</td>\n",
       "      <td>[[1], [0], [0], [0], [0], [0], [0], [0], [1], ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[1], [0], [0], [0], [0], [0], [1], [0], [0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ende 1940er Jahre wurde erste Auteur-Theorie f...</td>\n",
       "      <td>[[2], [1], [0], [1], [0], [1], [0], [2], [1], ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [1], [1], [1], [1], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al Pacino , geboren Manhattan , Sohn Salvatore...</td>\n",
       "      <td>[[4], [0], [2], [1], [2], [1], [0], [2], [4], ...</td>\n",
       "      <td>[[0], [0], [0], [1], [1], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[1], [0], [1], [1], [1], [1], [0], [0], [0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Der Name Alkalimetalle leitet arabischen Wort ...</td>\n",
       "      <td>[[1], [1], [0], [0], [1], [2], [1], [1], [1], ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [1], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Arbeit bereits seit Altertum Gegenstand re...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [2], [0], ...</td>\n",
       "      <td>[[0], [0], [0], [0], [0], [0], [0], [0], [0], ...</td>\n",
       "      <td>[[1], [1], [0], [1], [0], [0], [0], [0], [1], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             source  \\\n",
       "0           0  Minghella Sohn italienisch-schottischer Eltern...   \n",
       "1           1  Ende 1940er Jahre wurde erste Auteur-Theorie f...   \n",
       "2           2  Al Pacino , geboren Manhattan , Sohn Salvatore...   \n",
       "3           3  Der Name Alkalimetalle leitet arabischen Wort ...   \n",
       "4           4  Die Arbeit bereits seit Altertum Gegenstand re...   \n",
       "\n",
       "                                  no_words_inSent_SS  \\\n",
       "0  [[1], [0], [0], [0], [0], [0], [0], [0], [1], ...   \n",
       "1  [[2], [1], [0], [1], [0], [1], [0], [2], [1], ...   \n",
       "2  [[4], [0], [2], [1], [2], [1], [0], [2], [4], ...   \n",
       "3  [[1], [1], [0], [0], [1], [2], [1], [1], [1], ...   \n",
       "4  [[0], [0], [0], [0], [0], [0], [0], [2], [0], ...   \n",
       "\n",
       "                                  no_words_inSent_SK  \\\n",
       "0  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "1  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "2  [[0], [0], [0], [1], [1], [0], [0], [0], [0], ...   \n",
       "3  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "4  [[0], [0], [0], [0], [0], [0], [0], [0], [0], ...   \n",
       "\n",
       "                                            sim_sent  \n",
       "0  [[1], [0], [0], [0], [0], [0], [1], [0], [0], ...  \n",
       "1  [[0], [0], [0], [0], [0], [1], [1], [1], [1], ...  \n",
       "2  [[1], [0], [1], [1], [1], [1], [0], [0], [0], ...  \n",
       "3  [[0], [0], [0], [0], [0], [0], [0], [0], [1], ...  \n",
       "4  [[1], [1], [0], [1], [0], [0], [0], [0], [1], ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_features.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7dabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in nltk.word_tokenize(text) if word not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "df[\"source\"] = df[\"source\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b87243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85,stop_words=german_stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(df['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f4dfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c7ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.idf_\n",
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65d6d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af0617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['source'][2568]\n",
    "def get_kew(df):\n",
    "    kw=[]\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([df]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    for k in keywords:\n",
    "        kw.append(k)\n",
    "    return kw\n",
    "\n",
    "df['keywords'] = df['source'].apply(get_kew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "531614c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "def capital_lWords(df):\n",
    "    tokens = sent_tokenize(df, language=\"german\")\n",
    "    return list([list(sum([c.isupper() for c in a])) for a in tokens])\n",
    "\n",
    "def W_sourceSummary(df, df2):\n",
    "    f = []\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len(list((set(s.split()).intersection(set(tok2))))))\n",
    "    return [[el] for el in f]\n",
    "\n",
    "def W_sourceKeywords(df, df2):\n",
    "    f = []\n",
    "    df.lower()\n",
    "    text2 = df2\n",
    "    text2 = text2.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = sent_tokenize(df, language = \"german\")\n",
    "    tok2 = word_tokenize(text2, language = \"german\")\n",
    "    for s in tokens:\n",
    "        f.append(len(list((set(s.split()).intersection(set(tok2))))))\n",
    "    return [[el] for el in f]\n",
    "\n",
    "def listToString(df):\n",
    "    string = ' '.join([str(e) for e in df])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4546934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_words_inSent_SS'] = df.apply(lambda x: W_sourceSummary(x['source'], x['summary']), axis=1)\n",
    "df['keywords'] = df['keywords'].apply(listToString)\n",
    "df['no_words_inSent_SK'] = df.apply(lambda x: W_sourceKeywords(x['source'], x['keywords']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.no_words_inSent_SK[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ef8e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dorian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "H:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['al', 'au', 'bi', 'de', 'diesis', 'dy', 'e', 'mus', 'un', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet') # first-time use only\n",
    "german_stop_words = stopwords.words('german')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=german_stop_words)\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "\n",
    "def get_docs(df):\n",
    "    threshold = 0.15\n",
    "    doc = sent_tokenize(df, language=\"german\")\n",
    "    lst, tags, output_tuples, similar_sent = [], [], [], [] \n",
    "    arr = cos_similarity(doc)\n",
    "    doc_names = [\"s\"+str(s) for s in range(0, len(doc), 1)]\n",
    "    for row in range(len(arr)):\n",
    "        lst = [row+1+idx for idx, num in \\\n",
    "                  enumerate(arr[row, row+1:]) if num >= threshold]\n",
    "        [(output_tuples.append((doc[row], doc[item]))) for item in lst]\n",
    "    for tup in output_tuples:\n",
    "        for sent in tup:\n",
    "            similar_sent.append(sent)\n",
    "    for sent in doc:\n",
    "        if sent in set(doc).intersection(set(similar_sent)):\n",
    "            tags.append(1)\n",
    "        else:\n",
    "            tags.append(0)\n",
    "    return [[el] for el in tags]\n",
    "#get_docs(df['source'][0])\n",
    "df['sim_sent'] = df['source'].apply(get_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbe5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = df.drop(['summary', 'keywords'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d246de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d462c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([\"new york is a hell of a town\",\n",
    "                    \"new york was originally dutch\",\n",
    "                    \"new york is also called the big apple\",\n",
    "                    \"nyc is nice\",\n",
    "                    \"the capital of great britain is london. london is a huge metropolis which has a great many number of people living in it. london is also a very old town with a rich and vibrant cultural history.\",\n",
    "                    \"london is in the uk. they speak english there. london is a sprawling big city where it's super easy to get lost and i've got lost many times.\",\n",
    "                    \"london is in england, which is a part of great britain. some cool things to check out in london are the museum and buckingham palace.\",\n",
    "                    \"london is in great britain. it rains a lot in britain and london's fogs are a constant theme in books based in london, such as sherlock holmes. the weather is really bad there.\",])\n",
    "type(y_train)\n",
    "y_train.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54843c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "X_train = np.array(list(df.source[0]))\n",
    "y_train = np.array(df.sim_sent[0])\n",
    "\n",
    "X_test = np.array([sent_tokenize(df.source[5], language='german')])   \n",
    "\n",
    "\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(min_df=1,max_df=2,lowercase=False)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de68935",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927bd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['summary', 'keywords'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24eb46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data.source, data.sim_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b9840cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    tokens = sent_tokenize(row, language='german')\n",
    "    return tokens\n",
    "X = X.apply(identify_tokens)\n",
    "y = y.apply(identify_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b734e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y= np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b768028",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41528bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('german'))\n",
    "X = tfidfconverter.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f95929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c285726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b47ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2083ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "cls1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(min_df=5, max_df=0.7, stop_words=stopwords.words('german'),lowercase=False)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, random_state=0))])\n",
    "cls3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24c02a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('vectorizer',\n",
       "                                                                  CountVectorizer(lowercase=False,\n",
       "                                                                                  max_df=0.7,\n",
       "                                                                                  max_features=500,\n",
       "                                                                                  min_df=5,\n",
       "                                                                                  stop_words=['aber',\n",
       "                                                                                              'alle',\n",
       "                                                                                              'allem',\n",
       "                                                                                              'allen',\n",
       "                                                                                              'aller',\n",
       "                                                                                              'alles',\n",
       "                                                                                              'als',\n",
       "                                                                                              'also',\n",
       "                                                                                              'am',\n",
       "                                                                                              'an',\n",
       "                                                                                              'ander',\n",
       "                                                                                              'andere',\n",
       "                                                                                              'anderem',\n",
       "                                                                                              'anderen',\n",
       "                                                                                              'anderer',\n",
       "                                                                                              'anderes',\n",
       "                                                                                              'anderm',\n",
       "                                                                                              'andern',\n",
       "                                                                                              'anderr',\n",
       "                                                                                              'anders',\n",
       "                                                                                              'auch',\n",
       "                                                                                              'auf',\n",
       "                                                                                              'aus',\n",
       "                                                                                              'bei',\n",
       "                                                                                              'bin',\n",
       "                                                                                              'bis',\n",
       "                                                                                              'bist',\n",
       "                                                                                              'da',\n",
       "                                                                                              'damit',\n",
       "                                                                                              'dann', ...])),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                ('length',\n",
       "                                                 Pipeline(steps=[('count',\n",
       "                                                                  FunctionTransformer(func=<function get_text_length at 0x0000020665096B80>))]))])),\n",
       "                ('clf', SVC())])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "cls1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(max_features=500, min_df=5, max_df=0.7, stop_words=stopwords.words('german'),lowercase=False)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', svm.SVC())])\n",
    "cls1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4a54058",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultinomialNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5d7759ad6168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         ])),\n\u001b[0;32m     13\u001b[0m     ])),\n\u001b[1;32m---> 14\u001b[1;33m     ('clf', MultinomialNB())])\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mcls2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
     ]
    }
   ],
   "source": [
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "cls1 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorizer', CountVectorizer(max_features=500, min_df=5, max_df=0.7, stop_words=stopwords.words('german'),lowercase=False)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultinomialNB())])\n",
    "cls2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9994a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cls1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = cls2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8af020",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = cls3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(accuracy_score(y_test, y_pred3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
