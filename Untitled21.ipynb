{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, product\n",
    "import warnings, pandas as pd, nltk, string\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim, string\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import kullback_leibler, hellinger\n",
    "from gensim.models import ldamodel\n",
    "from gensim.similarities import MatrixSimilarity, SparseMatrixSimilarity, Similarity\n",
    "#from gensim.summarization.bm25 import get_bm25_weights\n",
    "from gensim.utils import simple_preprocess\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import spacy\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "import networkx as nx, algorithmx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427243b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/data_train.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed878913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def cosine_sim(sent1, sent2):\n",
    "    s1 = sent1.translate(str.maketrans('', '', string.punctuation))\n",
    "    s2 = sent2.translate(str.maketrans('', '', string.punctuation))\n",
    "    s1 = word_tokenize(sent1, language='german')\n",
    "    s2 = word_tokenize(sent2, language='german')\n",
    "    sw = stopwords.words('german')\n",
    "    l1, l2 = [],[]\n",
    "    \n",
    "    X_set = {w for w in s1 if not w in sw} \n",
    "    Y_set = {w for w in s2 if not w in sw}\n",
    "    \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector:\n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0)\n",
    "        if w in Y_set: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    c = 0\n",
    "\n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)):\n",
    "            c+= l1[i]*l2[i]\n",
    "    return c / float((sum(l1)*sum(l2))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbke_score(df, top_n, thresh):\n",
    "    sentences = sent_tokenize(df, language=\"german\")\n",
    "    g=nx.DiGraph()\n",
    "    edges = [(i,j,cosine_sim(x,y)) \n",
    "             for i,x in enumerate(sentences) \n",
    "             for j,y in enumerate(sentences) if i < j]\n",
    "\n",
    "    g.add_weighted_edges_from((i,j,sim) for i,j,sim in edges)\n",
    "    lst =[]\n",
    "    for i in g.nodes():\n",
    "        no_nei = len(g.edges(i))\n",
    "        if no_nei !=0 :\n",
    "            w_sum = g.degree(i,weight='weight')\n",
    "            w_sum += w_sum\n",
    "            alpha = 1-w_sum\n",
    "            sbke = no_nei*(w_sum/no_nei)**alpha\n",
    "            lst.append((sentences[i], sbke))\n",
    "        else:\n",
    "            lst.append((sentences[i], 0))\n",
    "\n",
    "    top = nlargest(top_n, lst, key=itemgetter(1))\n",
    "    top_s = [x[0] for x in top]\n",
    "    string = ' '.join([str(e) for e in top_s])\n",
    "    return string\n",
    "\n",
    "test_df = df.head(50)\n",
    "test_df['sbke_cos1'] = test_df.apply(lambda x: sbke_score(x['source'], 1, 0.15), axis=1)\n",
    "test_df['sbke_cos2'] = test_df.apply(lambda x: sbke_score(x['source'], 2, 0.15), axis=1)\n",
    "test_df['sbke_cos5'] = test_df.apply(lambda x: sbke_score(x['source'], 5, 0.15), axis=1)\n",
    "test_df['sbke_cos10'] = test_df.apply(lambda x: sbke_score(x['source'], 10, 0.15), axis=1)\n",
    "df.to_csv('G:/Extractive-Summarisation-of-German-Wikipedia/dataset/sbke_cos.csv', encoding='utf-8')\n",
    "\n",
    "from rouge import Rouge\n",
    "import sys\n",
    "\n",
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/sbke_cos_res.txt\", \"w\")\n",
    "\n",
    "hy, rf = test_df['sbke_cos1'], test_df['summary']\n",
    "rouge= Rouge()\n",
    "scoresT1= rouge.get_scores(hy, rf, avg=True)\n",
    "print(\"sbke_cos1 scores: \", scoresT1, '\\n\\n')\n",
    "\n",
    "hy2 = test_df['sbke_cos2']\n",
    "scoresT2= rouge.get_scores(hy2, rf, avg=True)\n",
    "print(\"sbke_cos2 scores: \", scoresT2, '\\n\\n')\n",
    "\n",
    "hy5 = test_df['sbke_cos5']\n",
    "scoresT5= rouge.get_scores(hy5, rf, avg=True)\n",
    "print(\"sbke_cos5 scores: \", scoresT5, '\\n\\n')\n",
    "\n",
    "hy10 = test_df['sbke_cos10']\n",
    "scoresT10= rouge.get_scores(hy10, rf, avg=True)\n",
    "print(\"sbke_cos10 scores: \", scoresT10, '\\n\\n')\n",
    "\n",
    "sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_Score(df, top_n, thresh):\n",
    "    sentences = sent_tokenize(df, language=\"german\")\n",
    "    g=nx.DiGraph()\n",
    "    edges = [(i,j,jaccard(x,y)) \n",
    "             for i,x in enumerate(sentences) \n",
    "             for j,y in enumerate(sentences) if i < j]\n",
    "\n",
    "    g.add_weighted_edges_from((i,j,sim) for i,j,sim in edges)\n",
    "    lst,res = [],[]\n",
    "    pr = nx.pagerank(g, weight='weight')\n",
    "    \n",
    "    for s, i in enumerate(sentences):\n",
    "        lst.append((i, pr[s]))\n",
    "    top = nlargest(top_n, lst, key=itemgetter(1))\n",
    "    top_s = [x[0] for x in top]\n",
    "    string = ' '.join([str(e) for e in top_s])\n",
    "    return (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191065c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "import sys\n",
    "\n",
    "sys.stdout = open(\"G:/Extractive-Summarisation-of-German-Wikipedia/dataset/sbke_cos_res.txt\", \"w\")\n",
    "\n",
    "hy, rf = test_df['sbke_cos1'], test_df['summary']\n",
    "rouge= Rouge()\n",
    "scoresT1= rouge.get_scores(hy, rf, avg=True)\n",
    "print(\"sbke_cos1 scores: \", scoresT1, '\\n\\n')\n",
    "\n",
    "hy2 = test_df['sbke_cos2']\n",
    "scoresT2= rouge.get_scores(hy2, rf, avg=True)\n",
    "print(\"sbke_cos2 scores: \", scoresT2, '\\n\\n')\n",
    "\n",
    "hy5 = test_df['sbke_cos5']\n",
    "scoresT5= rouge.get_scores(hy5, rf, avg=True)\n",
    "print(\"sbke_cos5 scores: \", scoresT5, '\\n\\n')\n",
    "\n",
    "hy10 = test_df['sbke_cos10']\n",
    "scoresT10= rouge.get_scores(hy10, rf, avg=True)\n",
    "print(\"sbke_cos10 scores: \", scoresT10, '\\n\\n')\n",
    "\n",
    "sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
